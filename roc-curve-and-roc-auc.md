# ROC-кривые и ROC AUC: Оценка Классификаторов

## Table of Contents

1. [Введение](#введение)
2. [Базовые Понятия: TP, FP, FN, TN](#базовые-понятия-tp-fp-fn-tn)
3. [Что Такое TPR и FPR](#что-такое-tpr-и-fpr)
4. [Определение ROC-кривой](#определение-roc-кривой)
5. [Определение ROC AUC и Интуиция](#определение-roc-auc-и-интуиция)
6. [Как Строить ROC и ROC AUC на Практике](#как-строить-roc-и-roc-auc-на-практике)
7. [Как Выбирать и Сравнивать Модели по ROC](#как-выбирать-и-сравнивать-модели-по-roc)
8. [Как Выбрать Порог Классификации](#как-выбрать-порог-классификации)
9. [ROC vs PR-кривые](#roc-vs-pr-кривые)
10. [Примеры Кода (Python, sklearn)](#примеры-кода-python-sklearn)
11. [Интуитивное Объяснение “для 5-летнего”](#интуитивное-объяснение-для-5-летнего)
12. [References](#references)

---

![image](https://upload.wikimedia.org/wikipedia/commons/3/3d/Roc_curve.svg)

## Введение

**ROC-кривая (Receiver Operating Characteristic)** и **ROC AUC (Area Under the ROC Curve)** — это классические инструменты для оценки качества **бинарных классификаторов**, особенно когда модель возвращает **скоры/вероятности**, а не только метки 0/1.

Идея:

- модель выдаёт для каждого объекта число $s(x) \in [0, 1]$ — насколько сильно он похож на класс 1;
- мы можем варьировать порог $t$, при котором говорим: “да, это класс 1”;
- ROC-кривая показывает, как меняются **чувствительность** (TPR) и **ложноположительная доля** (FPR) при всех возможных порогах;
- ROC AUC сворачивает **всю кривую** в одно число от 0 до 1, описывающее “общую способность модели различать классы”.

---

## Базовые Понятия: TP, FP, FN, TN

Для бинарной классификации (класс 0 и класс 1) вводятся четыре базовых количества:

- **TP (True Positives)** — истинно положительные: модель предсказала класс 1, и это действительно класс 1.
- **FP (False Positives)** — ложно положительные: модель предсказала класс 1, но на самом деле это класс 0.
- **TN (True Negatives)** — истинно отрицательные: модель предсказала класс 0, и это действительно класс 0.
- **FN (False Negatives)** — ложно отрицательные: модель предсказала класс 0, но на самом деле это класс 1.

Эти четыре числа полностью описывают поведение бинарного классификатора при **фиксированном пороге**.

Часто их удобно представлять в виде **confusion matrix**:

|                     | Истинный класс 1 | Истинный класс 0 |
|---------------------|------------------|------------------|
| Предсказан класс 1  | TP               | FP               |
| Предсказан класс 0  | FN               | TN               |

---

Precision: “Насколько чисты мои ПРЕДСКАЗАННЫЕ позитивы?”

Recall: “Какую долю всех настоящих позитивов я нашёл?”

Accuracy (достоверность): “Какую долю объектов в целом я классифицировал правильно?”

Specificity (избирательность): “Какую долю настоящих нулей я не перепутал с единицами?”



## Что Такое TPR и FPR

ROC-кривая строится в координатах **TPR–FPR**, поэтому критично понимать эти две величины.

### True Positive Rate (TPR, Sensitivity, Recall)

**TPR** показывает, какую долю объектов настоящего положительного класса модель **правильно поймала**:

$$
\text{TPR} = \frac{TP}{TP + FN}
$$

Интуитивно:

- из всех объектов класса 1 (настоящих “позитивов”) какая часть помечена моделью как 1.

### False Positive Rate (FPR)

**FPR** показывает, какую долю объектов отрицательного класса модель **ошибочно** назвала положительными:

$$
\text{FPR} = \frac{FP}{FP + TN}
$$

Интуитивно:

- среди всех настоящих нулей (класс 0) какая часть модель **перепутала с единицами**.

Важно:

- TPR и FPR **зависят от выбранного порога** $t$;
- при очень низком пороге (почти всё считаем классом 1) TPR будет высоким, но FPR тоже вырастет;
- при очень высоком пороге TPR и FPR оба будут близки к 0.

---

## Определение ROC-кривой

Пусть модель выдаёт скор $s(x) \in [0, 1]$ (часто — оценка вероятности класса 1). Чтобы получить предсказание 0/1, мы выбираем порог $t$:

- если $s(x) \ge t$ → предсказываем класс 1;
- если $s(x) < t$ → предсказываем класс 0.

**ROC-кривая** строится так:

1. Перебираем множество порогов $t$ в диапазоне $[0, 1]$ (на практике — все уникальные значения скоров).
2. Для каждого порога $t$ считаем $TP, FP, FN, TN$.
3. Вычисляем:
   - $\text{TPR}(t)$,
   - $\text{FPR}(t)$.
4. Строим точки $(\text{FPR}(t), \text{TPR}(t))$ на плоскости.

Оси:

- по оси X: **FPR** от 0 до 1;
- по оси Y: **TPR** от 0 до 1.

Типичные формы:

- **идеальная модель** — кривая проходит близко к точке $(0, 1)$, почти **в левом верхнем углу**;
- **случайная модель** (монетка) — ROC лежит около диагонали от $(0, 0)$ до $(1, 1)$;
- **плохая модель** — ROC может располагаться ниже диагонали (иногда это значит, что стоит просто инвертировать предсказания).

---

## Определение ROC AUC и Интуиция

**ROC AUC (Area Under the ROC Curve)** — это **площадь под ROC-кривой**:

- принимает значения в диапазоне $[0, 1]$;
- $1.0$ — идеальный классификатор;
- $0.5$ — модель не лучше случайного угадывания;
- $< 0.5$ — модель “перепутала” классы (если инвертировать предсказания, AUC станет > 0.5).

Формально, это можно записать как интеграл:

$$
\text{AUC} = \int_0^1 \text{TPR}(\text{FPR}) \, d(\text{FPR})
$$

### Важная вероятностная интерпретация

ROC AUC можно интерпретировать так:

> **ROC AUC — это вероятность того, что случайно выбранный объект класса 1 получит моделью более высокий скор, чем случайно выбранный объект класса 0.**

То есть:

$$
\text{AUC} = P\left( s(x^+) > s(x^-) \right),
$$

где $x^+$ — объект позитивного класса, $x^-$ — объект негативного класса.

Эта интерпретация подчёркивает, что ROC AUC — это **метрика ранжирования**, а не качества при одном конкретном пороге.

---

## Как Строить ROC и ROC AUC на Практике

На практике чаще всего используют готовые функции (например, в `sklearn`), но полезно понимать, что происходит “под капотом”.

### Входные данные

Нам нужны:

- `y_true`: вектор истинных меток из $\{0, 1\}$;
- `y_score`: вектор скоров/вероятностей для класса 1 (выход модели до порога).

### Генерация ROC-кривой

1. Сортируем объекты по убыванию `y_score`.
2. Последовательно “сдвигаем” порог от $+\infty$ к $-\infty$:
   - на каждом шаге считаем, сколько среди отнесённых к классу 1 объектов стало TP, FP;
   - считаем TPR и FPR;
   - добавляем точку на ROC.

На практике библиотеки:

- перебирают **уникальные значения** `y_score` как пороги;
- возвращают массивы `fpr`, `tpr`, `thresholds`.

### Вычисление AUC

Имея массивы `fpr` и `tpr`, AUC можно приблизить численно:

- **метод трапеций** (линейная интерполяция между точками),
- готовые функции (`sklearn.metrics.roc_auc_score`).

---

## Как Выбирать и Сравнивать Модели по ROC

Предположим, есть несколько моделей (например, логистическая регрессия, Random Forest, XGBoost). Как решить, какая лучше, глядя на ROC?

### 1. Сравнение ROC-кривых визуально

Выводим ROC-кривые всех моделей на **один график**:

- если **кривая одной модели лежит выше** другой почти на всём диапазоне FPR — эта модель **строго лучше** по ROC;
- если кривые пересекаются, нужно понимать **какой диапазон FPR важен именно в вашей задаче**:
  - в медицине часто важно работать при очень малых FPR (например, $\text{FPR} \le 0.01$),
  - в рекламе или рекомендательных системах допустим более высокий FPR.

### 2. Сравнение по числовому ROC AUC

Смотрим на одно число:

- **чем больше AUC, тем лучше модель отделяет класс 1 от класса 0 в среднем по порогам**;
- разница 0.78 vs 0.82 обычно значимее, чем 0.95 vs 0.96 — важно учитывать размер выборки и статистическую значимость;
- ROC AUC особенно полезен, когда бизнес не зафиксировал ещё явный порог и важно общее качество ранжирования (например, кредитный скоринг, фрод-детекция).

Эмпирно:

- AUC < 0.6 — очень слабая модель;
- 0.6–0.7 — слабая, но может быть лучше заглушки;
- 0.7–0.8 — приемлемое качество;
- 0.8–0.9 — хорошая модель;
- > 0.9 — очень сильная модель (иногда стоит проверить утечки данных).

---

## Как Выбрать Порог Классификации

ROC-кривая показывает поведение модели **для всех порогов**, но при внедрении нужна **одна конкретная граница**.

Типичные стратегии:

### 1. Максимизация статистики Youden’s J

Определим:

$$
J(t) = \text{TPR}(t) - \text{FPR}(t).
$$

Выбираем порог:

$$
t^* = \arg\max_t J(t).
$$

Интерпретация:

- оптимальный компромисс между **чувствительностью** (TPR) и **специфичностью** $(1 - \text{FPR})$.

### 2. Ограничение FPR (контроль ложных тревог)

Если ложноположительные ошибки крайне дороги (например, необоснованная блокировка честных пользователей), задаём ограничение:

- “хотим $\text{FPR} \le \alpha$ (например, 0.01)”.

Тогда:

1. смотрим на все точки ROC с $\text{FPR}(t) \le \alpha$;
2. среди них выбираем порог с максимальным TPR (или максимальной бизнес-метрикой).

### 3. Оптимизация по F1 или кастомной функции потерь

Иногда важна не только ROC, но и баланс Precision/Recall или конкретные стоимости ошибок:

- можно вычислить для разных порогов:
  - Precision, Recall, F1;
  - бизнес-метрику (например, прибыль, экономию, NPV);
- выбрать порог, максимизирующий эту функцию.

ROC в этом случае:

- даёт понимание структуры ошибок;
- помогает ограничить диапазон интересных порогов (например, только там, где FPR достаточно мал).

---

## ROC vs PR-кривые

Помимо ROC, часто рассматривают **PR-кривую (Precision–Recall)**:

- по оси X: Recall (то же, что TPR);
- по оси Y: Precision (точность).

### Когда лучше ROC

- классы относительно **сбалансированы**;
- важно поведение на **всем диапазоне FPR**;
- интересно общее отделение классов, а не только работа с позитивным классом.

### Когда лучше PR

- при **сильном дисбалансе** (например, 1% позитивов и 99% негативов) ROC AUC может быть обманчиво высокой;
- PR-кривая более чувствительна к качеству обнаружения **редкого положительного класса**;
- в задачах фрода, редких заболеваний и т.д. PR AUC часто информативнее.

Практически:

- обычно смотрят **и ROC AUC, и PR AUC**;
- ROC — про способность различать классы,
- PR — про качество именно **пойманных позитивов** (их “чистоту” и полноту).

---

## Примеры Кода (Python, sklearn)

### Базовый расчёт ROC и ROC AUC

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# y_true: массив истинных меток (0/1)
# y_score: массив скоров/вероятностей для класса 1

fpr, tpr, thresholds = roc_curve(y_true, y_score)
auc = roc_auc_score(y_true, y_score)
print("ROC AUC:", auc)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f"ROC curve (AUC = {auc:.3f})")
plt.plot([0, 1], [0, 1], "k--", label="Random")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC-кривая")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### Поиск порога с максимальным Youden’s J

```python
import numpy as np
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_true, y_score)

# Youden's J = TPR - FPR
J = tpr - fpr
ix = np.argmax(J)
best_threshold = thresholds[ix]

print(f"Лучший порог по Youden's J: {best_threshold:.4f}")
print(f"TPR = {tpr[ix]:.3f}, FPR = {fpr[ix]:.3f}")
```

### Ограничение FPR сверху

```python
max_fpr = 0.01  # хотим FPR не больше 1%

candidates = np.where(fpr <= max_fpr)[0]
if len(candidates) > 0:
    best_idx = candidates[np.argmax(tpr[candidates])]
    threshold_fpr_constraint = thresholds[best_idx]
    print(f"Порог при FPR <= {max_fpr}: {threshold_fpr_constraint:.4f}")
    print(f"TPR = {tpr[best_idx]:.3f}, FPR = {fpr[best_idx]:.3f}")
else:
    print("Не удалось достичь такого малого FPR с данной моделью.")
```

---

## Интуитивное Объяснение “для 5-летнего”

Представь, что у тебя есть корзина с **яблоками** (класс 1) и **апельсинами** (класс 0), и у тебя есть машинка, которая каждому фрукту ставит оценку от 0 до 1 — **насколько он похож на яблоко**.

- Если мы скажем: “яблоки — это всё, у чего оценка ≥ 0.2”, мы получим **много яблок**, но и часть апельсинов ошибочно окажется среди яблок.
- Если мы скажем: “яблоки — только те, у кого оценка ≥ 0.9”, мы почти не перепутаем апельсины, но и часть настоящих яблок потеряем.

**ROC-кривая** рисует картинку, как меняется:

- сколько **настоящих яблок** мы нашли (TPR),
- сколько **апельсинов мы перепутали с яблоками** (FPR),

при всех возможных уровнях строгости.

**ROC AUC** — это одно число, которое показывает:

> насколько хорошо машинка в среднем ставит **яблокам** оценки выше, чем **апельсинам**.

Чем больше это число (ближе к 1), тем лучше машинка различает фрукты.

---

## References

### Related Documents

- **[Classification Losses: Cross-Entropy, Focal Loss](./classification-losses-cross-entropy-focal-loss.md)** — как функции потерь связаны с вероятностями и качеством классификаторов.
- **[Gaussian Distribution](./gaussian-distribution.md)** — базовая вероятность, на которой основаны многие статистические методы.
- **[Retrieval-Augmented Generation (RAG)](./retrieval-augmented-generation-rag.md)** — пример задач, где ранжирование и метрики качества тоже важны (хотя и другие).

### Key Concepts

- **Binary Classification**: бинарная классификация, confusion matrix.
- **Evaluation Metrics**: TPR/Recall, FPR, Precision, F1-score.
- **Ranking Metrics**: ROC AUC, PR AUC.

### Further Reading

1. **Fawcett (2006)**: “An Introduction to ROC Analysis”.
2. **Bishop (2006)**: "Pattern Recognition and Machine Learning" — глава про оценку качества классификаторов.
3. **sklearn Documentation**: раздел `metrics`, функции `roc_curve`, `roc_auc_score`.

---

*Документ создан: 2026*  
*Последнее обновление: 2026*

