## Table of Contents
- Введение
- Объяснение для 5‑летнего
- Что такое токенизатор
- Зачем вообще нужен токенизатор (и почему это сжатие)
- Основные типы токенизации
  - Токенизация по словам и пробелам
  - Токенизация по символам
  - Сабворд‑токенизация (subword)
    - BPE (Byte Pair Encoding)
    - Byte‑level BPE
    - WordPiece
    - Unigram LM (SentencePiece)
- Как именно текст превращается в числа в LLM
- Компрессия и выбор словаря токенов
- Практические рекомендации и грабли
- References

## Объяснение для 5‑летнего

Представь, что у тебя есть сказка, а у робота в голове только числа. Токенизатор — это умный резак текста, который режет слова и кусочки слов на маленькие кирпичики и каждому кирпичику даёт номер. Потом робот смотрит не на буквы, а на эти номера и учится говорить и писать, как человек.

## Введение

В любой современной языковой модели (LLM) перед самим трансформером прячется важный, но часто недооценённый модуль — **токенизатор**.  
Его задача — превратить сырой текст (строку символов) в **последовательность целых чисел**, с которой уже может работать модель.

Высокоуровневая схема всегда одинакова:

1. Сырой текст (строка, Unicode)
2. Токенизация: разбиение строки на токены (кусочки текста)
3. Преобразование токенов в ID из фиксированного словаря
4. Преобразование ID в эмбеддинги (вектора) и подача в Transformer

Правильная токенизация — это скрытый **алгоритм сжатия текста**, который напрямую влияет на:

- длину последовательностей (и, следовательно, стоимость внимания $O(n^2)$),
- размер словаря (и размер матрицы эмбеддингов),
- способность модели обрабатывать новые слова, редкие формы и многоязычный текст.

В этом конспекте разберём, что такое токенизатор, какие виды токенизации существуют и почему subword‑подходы (BPE, WordPiece, Unigram) стали стандартом де‑факто для LLM.

## Что такое токенизатор

**Токенизатор** — это алгоритм, который отображает строку текста в последовательность токенов, а затем в последовательность целых чисел (ID).

Пример (условный):

- Вход: `"Привет, как дела?"`
- Токены: `["Пр", "ивет", ",", "Ġкак", "Ġдел", "а", "?"]`
- ID: `[10342, 9121, 11, 845, 12987, 42, 30]`

Дальше каждый ID подаётся в матрицу эмбеддингов $E \in \mathbb{R}^{V \times d}$:

- $V$ — размер словаря (например, 50k токенов),
- $d$ — размерность эмбеддинга (например, 4096),
- каждая строка $E_i$ — вектор для токена с ID $i$.

Токенизатор определяет, что считать **“атомом смысла”**: слово, кусок слова или отдельный символ/байт.

## Зачем нужен токенизатор и почему это сжатие

Простое “взять каждый символ как отдельный ID” было бы корректно, но:

- последовательности были бы очень длинными,
- модели было бы сложнее схватывать смысл на уровне слов и фраз.

С другой стороны, “одно слово = один токен”:

- приводит к гигантскому словарю (миллионы словоформ для морфологически богатых языков),
- создаёт проблему OOV (out‑of‑vocabulary) — неизвестные слова нельзя закодировать,
- плохо работает для языков без пробелов (китайский, японский).

**Subword‑токенизация** ищет компромисс:

- частые куски текста кодируются **коротко** (часто одним токеном),
- редкие слова разбиваются на несколько субвордов или символов.

Это ведёт себя как **энтропийное сжатие**:

- частые паттерны получают “короткий код” (мало токенов),
- редкие паттерны — “длинный код”.

Алгоритмы типа BPE/WordPiece/Unigram фактически **учатся словарю сжатия** на большом корпусе, минимизируя среднюю длину разметки текста в токены.

## Основные типы токенизации

### Токенизация по словам и пробелам

Исторически первые и самые простые подходы:

- **Whitespace токенизация**:
  - режем строку по пробелам и пунктуации;
  - `"Hello, world!"` → `["Hello", ",", "world", "!"]`.
- **Word‑level токенизаторы**:
  - фиксированный словарь слов, каждое слово — отдельный токен;
  - `"playing"`, `"played"`, `"plays"` — три разных токена.

Плюсы:

- легко реализовать;
- человеко‑понятная интерпретация (токен ≈ слово).

Минусы:

- огромный словарь для реальных корпусов;
- слова вне словаря (OOV) вообще нельзя представить;
- плохо для морфологических языков (русский, турецкий и т.п.);
- не подходит для языков без пробелов.

### Токенизация по символам

**Character‑level**: каждый символ — токен.

- `"Привет"` → `["П", "р", "и", "в", "е", "т"]`.

Плюсы:

- любой текст кодируем без OOV;
- словарь небольшой (буквы, цифры, знаки, спецсимволы).

Минусы:

- длина последовательности растёт в разы;
- модели сложнее учиться на “сырых” символах, чтобы улавливать смыслы на уровне слов и фраз;
- вычислительные затраты на внимание сильно возрастают.

### Сабворд‑токенизация (subword)

Ключевая идея: **учим словарь кусочков слов** (subwords), который:

- компактен (тысячи–десятки тысяч токенов),
- даёт короткие разложения для частых слов и устойчивых морфем,
- позволяет собрать любое новое слово как конкатенацию токенов.

Сегодня почти все крупные LLM используют одну из форм сабворд‑токенизации.

#### BPE (Byte Pair Encoding)

Классический BPE:

1. Берём алфавит базовых токенов (символы или байты).
2. Считаем частоты всех пар последовательных токенов на большом корпусе.
3. Находим самую частую пару и **сливаем её в новый токен**.
4. Повторяем шаг 2–3, пока не наберём желаемый размер словаря.

В итоге:

- распространённые слова/морфемы становятся одиночными токенами;
- редкие слова разбиваются на несколько субвордов.

Такой подход реализован (с вариациями) в семействе GPT‑моделей.

#### Byte‑level BPE

В GPT‑2 и многих современных моделях BPE работает не по “символам Unicode”, а по **байтам** (0–255):

- любой Unicode‑текст можно однозначно представить как последовательность байтов;
- не нужны специальные правила для редких символов и эмодзи;
- получается универсальный токенизатор для произвольных строк.

Плюсы:

- отсутствие проблем с кодировками и OOV на уровне символов;
- удобен для мультиязычности и смешанных текстов (код + естественный язык).

Минусы:

- токены могут быть “нечитаемыми” для человека (байтовые последовательности);
- отладка и интерпретация сложнее.

#### WordPiece

Используется в BERT, RoBERTa и их производных.

Идея похожа на BPE, но выбор новых токенов оптимизирует вероятностную цель: максимизацию правдоподобия корпуса при языковой модели на сабвордах.

Особенности:

- часто используется спецпрефикс (например, `##ing`, `##tion`) для продолжений слова;
- пример: `"playing"` → `["play", "##ing"]`.

Плюсы:

- хорошее покрытие лексики;
- устойчивость к новым словам.

#### Unigram Language Model (SentencePiece)

Реализован в `SentencePiece` и используется, например, в T5.

Идея:

1. Начинаем с большого набора кандидатов‑сабвордов.
2. Строим вероятностную модель, которая задаёт распределение разбиений текста на эти сабворды.
3. Итеративно удаляем те токены, которые мало помогают описывать корпус (почти не используются или ухудшают правдоподобие).

В отличие от BPE, где мы **последовательно добавляем** пары, Unigram LM **последовательно удаляет** неэффективные токены, сохраняя те, которые наиболее полезны.

Плюсы:

- гибкая вероятностная модель разбиения;
- удобен для многоязычных корпусов и сложных сценариев.

#### SentencePiece как библиотека

`SentencePiece` — это библиотека от Google, которая:

- работает прямо с Unicode/байтами (не требует предварительной токенизации по словам),
- реализует и BPE, и Unigram LM,
- широко используется в современных моделях (T5, mT5 и др.).

## Как текст превращается в числа в LLM

Пайплайн для одной строки (упрощённо):

1. **Нормализация текста**
   - приведение к Unicode NFC/NFKC,
   - опциональное lowercasing,
   - частичная очистка (зависит от конкретного токенизатора).

2. **Токенизация**
   - `"How text becomes numbers?"` → `["How", "Ġtext", "Ġbecome", "s", "Ġnumber", "s", "?"]`
   - точное разбиение зависит от словаря и алгоритма (BPE/WordPiece/Unigram).

3. **Преобразование в ID**
   - каждому токену соответствует целочисленный индекс из словаря:
   - `["How", "Ġtext", ...]` → `[1234, 987, 56, 41, 829, 41, 17]`.

4. **Эмбеддинги**
   - есть матрица эмбеддингов $E \in \mathbb{R}^{V \times d}$;
   - для каждого ID $i$ берём строку $E_i$;
   - получаем последовательность векторов $[\mathbf{e}_{1234}, \mathbf{e}_{987}, \dots]$.

5. **Позиционные/роторные эмбеддинги**
   - к вектору токена добавляется/встраивается информация о его позиции (absolute/relative/RoPE/2D‑RoPE и т.д.).

6. **Transformer**
   - дальше работает стандартная архитектура (self‑attention, MLP‑слои);
   - модель предсказывает распределение по словарю для **следующего токена**.

Таким образом, **всё общение с LLM идёт через пространство токенов**, а не через “сырые строки”.

## Компрессия и выбор словаря токенов

Качество токенизатора можно грубо оценивать по:

- средней длине последовательности токенов для корпуса;
- способности адекватно кодировать редкие слова и новые термины;
- размере словаря и, как следствие, размере матрицы эмбеддингов.

Если словарь слишком маленький:

- последовательности слишком длинные,
- растёт стоимость внимания и сложность обучения.

Если словарь слишком большой:

- растёт матрица эмбеддингов ($V \times d$),
- больше памяти и параметров,
- сложнее обучить хорошие эмбеддинги для редко встречающихся токенов.

Алгоритмы вроде BPE/Unigram фактически решают **компромиссную задачу**:

- минимизировать среднюю длину разложения текстов на токены
- при фиксированном (или ограниченном) размере словаря.

## Практические рекомендации и грабли

- **Не смешивать токенизаторы**:
  - модель обучалась с фиксированным токенизатором; менять его на другой почти всегда разрушает качество;
  - даже разные версии BPE/WordPiece с другим словарём уже несовместимы.

- **Учитывать язык и домен**:
  - для морфологически богатых языков и домен‑специфической лексики (медицина, код, формулы) выгодно обучать словарь на профильном корпусе;
  - иначе важные термины могут всегда разбиваться на длинные цепочки токенов.

- **Byte‑level BPE** — хороший дефолт:
  - стабильно работает с любым Unicode;
  - упрощает обработку смешанных текстов (естественный язык + код).

- **Оценивать распределение длин**:
  - полезно смотреть гистограммы длины токенизированных последовательностей для целевого корпуса;
  - если хвост слишком тяжёлый — возможно, словарь недостаточно хорошо подобран.

- **Помнить про стоимость внимания**:
  - два токенизатора с одинаковым качеством, но разным средним числом токенов на предложение могут сильно различаться по стоимости инференса (из‑за квадратичной сложности по длине).

## References

- Внутри `./knowledge-book`:
  - [Embeddings and Embedding Matrix](./embeddings-and-embedding-matrix.md) — что такое эмбеддинги, матрица эмбеддингов и как они связаны с токенизатором.
  - [Transformers, Attention and Vision Transformers (ViT)](./transformers-attention-and-vision-transformers-vit.md) — архитектура трансформеров, attention, RoPE.
  - [Retrieval-Augmented Generation (RAG)](./retrieval-augmented-generation-rag.md) — RAG‑системы и работа LLM с внешними знаниями.
  - [Low-Rank Adaptation (LoRA)](./low-rank-adaptation-lora.md) — дообучение LLM, где токенизация остаётся фиксированной.
- Внешние материалы:
  - Sennrich, Haddow, Birch — “Neural Machine Translation of Rare Words with Subword Units” (BPE).
  - Kudo — “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”.
  - Kudo, Richardson — “SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing”.

