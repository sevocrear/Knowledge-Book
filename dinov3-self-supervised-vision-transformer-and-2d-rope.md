## DINOv3: Self-Supervised Vision Transformer и 2D RoPE

## Table of Contents

1. Краткий абстракт и объяснение для 5‑летнего ребёнка  
2. Интуиция DINOv3: зачем ещё один ViT‑фреймворк  
3. Архитектура DINOv3 как ViT‑бэкбон  
4. Self-Supervised обучение: student–teacher, multi‑view, multi‑loss  
5. 2D Rotary Positional Embeddings (2D RoPE) для изображений  
6. Как формируются фичи: глобальные и dense‑представления  
7. Применение DINOv3 для классификации  
8. Применение DINOv3 для детекции и сегментации  
9. Связанные темы и References внутри knowledge‑book  

---

### 1. Краткий абстракт и объяснение для 5‑летнего ребёнка

**Кратко по‑взрослому.**  
**DINOv3** — это большое семейство self‑supervised **Vision Transformer**‑моделей, которые обучаются **без разметки** изображений и учатся извлекать сильные **универсальные визуальные фичи**. Они используют:
- обычную архитектуру ViT (патчи, self‑attention, LayerNorm и т.д.),
- схему **student–teacher** с несколькими аугментированными кропами одного и того же изображения,
- комбинированные лоссы на **глобальных** и **патч‑уровневых** признаках (DINO‑loss + iBOT‑подобный patch‑loss + регуляризации),
- и **2D Rotary Positional Embeddings (2D RoPE)**, чтобы аккуратно кодировать координаты патчей в self‑attention и устойчиво масштабироваться на высокие разрешения.

После такого pretraining’а DINOv3‑бэкбон можно использовать:
- как почти «drop‑in» замену CNN‑бэкбона для **классификации, детекции, сегментации, depth estimation и других dense‑задач**;
- как foundation‑модель для дальнейшего fine‑tuning или линейного probe’а.

**Как объяснить 5‑летнему ребёнку.**  
Представь, что у тебя есть огромная книга с миллионами картинок, но никто не подписал, что на них нарисовано.  
DINOv3 — это очень умный «художник‑детектив», который:
- режет каждую картинку на маленькие квадратики‑кусочки,  
- учится понимать, какие кусочки похожи между собой, даже если они из разных картинок,  
- и запоминает, **где именно** лежит каждый кусочек на картинке (слева‑справа, сверху‑снизу) с помощью особой «сеточки координат» (2D RoPE).  
Потом, когда ты даёшь ему новую картинку и говоришь «скажи, что здесь нарисовано» или «покажи, где кот», он уже умеет:
- узнавать объекты,
- обводить их рамками,
- и даже раскрашивать каждый пиксель по классам.

---

### 2. Интуиция DINOv3: зачем ещё один ViT‑фреймворк

Основные цели DINOv3:

- **Self‑supervised pretraining на огромных объёмах данных**  
  Модель учится на миллиардах изображений **без меток**, то есть без явных классов, боксов или масок.  
  Вместо этого она учится согласовывать представления разных аугментаций одного и того же изображения.

- **Сильные dense‑фичи для CV‑задач**  
  DINOv1/v2 уже показали, что self‑supervised ViT‑фичи хорошо работают для сегментации и детекции.  
  DINOv3 идёт дальше:
  - улучшает стабильность и качество **patch‑уровневых признаков**,
  - добавляет специальные регуляризации (например, Gram Anchoring, Koleo‑подобные подходы) для борьбы с деградацией dense‑фичей при долгом обучении.

- **Хорошая масштабируемость по разрешению и размеру модели**  
  Использование **2D RoPE** и аккуратной схемы позиционирования позволяет:
  - тренировать модель на одних разрешениях,  
  - а потом применять на более высоких (или немного других) без «ломания» позиционного кодирования.

Итог: DINOv3 — это «универсальный ViT‑бэкбон», который:
- обучен self‑supervised,
- хорошо переносится на новые датасеты и задачи,
- выдаёт фичи, пригодные как для глобальной классификации, так и для локальных dense‑предсказаний.

---

### 3. Архитектура DINOv3 как ViT‑бэкбон

С точки зрения архитектуры, DINOv3 — это **вариант Vision Transformer (ViT)** с некоторыми инженерными нюансами:

1. **Patch Embedding**  
   Вход: изображение размера $H \times W \times 3$ (обычно RGB).
   - Делим на патчи размера $P \times P$.
   - Получаем решётку патчей размера:
     $$
     \frac{H}{P} \times \frac{W}{P}.
     $$
   - Каждый патч прогоняется через линейную проекцию (как правило, `Conv2d` с ядром $P$ и шагом $P$), получаем вектор
     $$
     \mathbf{e}_{x,y} \in \mathbb{R}^D,
     $$
     где $(x, y)$ — индексы патча по сетке.

2. **Последовательность токенов**  
   Все патчи раскладываются в последовательность токенов:
   $$
   X = [\mathbf{e}_{1,1}, \mathbf{e}_{1,2}, \dots, \mathbf{e}_{h,w}] \in \mathbb{R}^{N \times D},
   $$
   где $h = \frac{H}{P}$, $w = \frac{W}{P}$, $N = h \cdot w$.
   - Опционально может быть `CLS`‑токен, но для многих dense‑задач часто используют **пуллинг по патчам**.

3. **Позиционная информация через 2D RoPE**  
   Вместо явного сложения $X + P$ с позиционным эмбеддингом:
   - расчитываются координаты $(x, y)$ для каждого токена,
   - при вычислении **self‑attention** к векторам $Q, K$ применяется **2D Rotary Positional Embedding**, зависящий от $(x, y)$.
   Подробнее об этом — в разделе 5.

4. **Трансформер‑энкодер**  
   Набор одинаковых блоков:
   - Multi‑Head Self‑Attention (MHA) с 2D RoPE,
   - MLP (feedforward),
   - residual connections,
   - LayerNorm (обычно в pre‑norm конфигурации).

5. **Выходные признаки**  
   На выходе каждого слоя имеем:
   $$
   Z^{(l)} \in \mathbb{R}^{N \times D}.
   $$
   - Для **классификации** часто берут:
     - либо CLS‑токен,
     - либо глобальный пуллинг по патчам:
       $$
       \mathbf{z}_{\text{global}} = \frac{1}{N} \sum_{i=1}^N \mathbf{z}^{(L)}_i.
       $$
   - Для **детекции/сегментации** берут **dense‑карту признаков**:
     - reshape $Z^{(L)}$ обратно в тензор $h \times w \times D$ и подают в декодер/детектор.

См. общий обзор архитектуры трансформеров и ViT в документе  
**`transformers-attention-and-vision-transformers-vit.md`**.

---

### 4. Self-Supervised обучение: student–teacher, multi‑view, multi‑loss

Обучение DINOv3 следует идеям **self‑distillation** и **multi‑view contrastive/distillation learning**:

1. **Student–Teacher схема**
   - Есть две модели с одинаковой архитектурой: **student** и **teacher**.
   - **Teacher** не обучается градиентом напрямую: его веса — это **экспоненциальное скользящее среднее (EMA)** весов student’а:
     $$
     \theta_{\text{teacher}} \leftarrow \tau \theta_{\text{teacher}} + (1 - \tau)\theta_{\text{student}},
     $$
     где $\tau \in [0, 1)$ — коэффициент сглаживания.
   - Такая схема предотвращает коллапс и даёт стабильные «псевдо‑таргеты».

2. **Multi‑view аугментации**
   - Из одного исходного изображения генерируется несколько аугментированных «видов»:
     - 2–3 **глобальных кропа** (большая часть изображения),
     - несколько **локальных кропов** (маленькие вырезанные участки).
   - Эти кропы подаются:
     - в teacher (обычно глобальные),
     - в student (глобальные + локальные).
   - Идея: все виды **одного и того же изображения** должны иметь согласованные представления.

3. **Глобальный DINO‑loss**
   - Пусть teacher даёт для каждого global‑view вектор/логиты $t$, student — $s$.
   - На них накладывается **softmax c температурой** и **center‑trick** (вычитание обучаемого центра), далее считается cross‑entropy или KL‑дивергенция:
     $$
     \mathcal{L}_{\text{DINO}} = \text{CE}\big(\text{softmax}(t / T_t), \text{softmax}(s / T_s)\big).
     $$
   - Это заставляет **глобальные** эмбеддинги разных кропов одного и того же изображения совпадать.

4. **Patch‑level / iBOT‑style loss**
   - Дополнительно DINOv3 использует **патч‑уровневый loss** (в духе iBOT):
     - некоторые патчи (или их представления) «маскируются» или выбрасываются,
     - student должен **восстановить** teacher‑признаки для этих патчей.
   - Это делает **локальные dense‑фичи информативными** для каждого патча, а не только для CLS/глобального вектора.

5. **Регуляризации (Koleo, Gram Anchoring и др.)**
   - При долгом обучении на огромных датасетах есть риск, что:
     - глобальные фичи остаются хорошими,
     - но **dense‑фичи деградируют** (становятся слишком коррелированными или «плоскими»).
   - Поэтому вводятся дополнительные регуляризации:
     - **Koleo‑подобные** — тянут распределение признаков к более равномерному по сфере, борются с коллапсом,
     - **Gram Anchoring** — стабилизирует **матрицы Грама** каналов (корреляции каналов), чтобы структура dense‑фич по каналам не разрушалась.

В совокупности:
- DINO‑loss выравнивает **глобальные** представления,
- patch‑loss и регуляризации улучшают **локальные** dense‑фичи.

---

### 5. 2D Rotary Positional Embeddings (2D RoPE) для изображений

Перед чтением этого раздела полезно освежить базовые идеи позиционного кодирования и классического **RoPE** в документе  
**`transformers-attention-and-vision-transformers-vit.md`**, раздел «Позиционное кодирование».

#### 5.1. Напоминание: 1D RoPE

В 1D RoPE (как в LLaMA и др.) позиция $p$ кодируется не сложением вектора, а **вращением** пар компонент векторов $Q$ и $K$.

Пусть у нас есть пара компонент $(q_{2i}, q_{2i+1})$ в одном attention‑голове. Для позиции $p$ задаём угол $\theta_i(p)$, и применяем поворот:
$$
\begin{pmatrix}
q'_{2i} \\
q'_{2i+1}
\end{pmatrix}
=
\begin{pmatrix}
\cos \theta_i(p) & -\sin \theta_i(p) \\
\sin \theta_i(p) & \cos \theta_i(p)
\end{pmatrix}
\begin{pmatrix}
q_{2i} \\
q_{2i+1}
\end{pmatrix},
$$
аналогично для компонент $K$.

Интуиция:
- относительное смещение позиций $p' - p$ отражается в **разности фаз** поворотов,
- attention естественным образом кодирует **относительные позиции**, а не только абсолютные индексы.

#### 5.2. 2D RoPE: кодируем $(x, y)$

Для изображений каждая позиция — это не одно число $p$, а пара **координат**:
$$
(x, y), \quad x \in \{0, \dots, h-1\},\quad y \in \{0, \dots, w-1\},
$$
где $h, w$ — размеры решётки патчей.

Идея 2D RoPE:

1. **Разделить каналы вектора на подпространства для $x$ и $y$**  
   Пусть вектор $q \in \mathbb{R}^D$ (одна голова или часть головы).
   - Первую половину каналов условно ассоциируем с **горизонтальной координатой $x$**.
   - Вторую половину — с **вертикальной координатой $y$**.

2. **Определить углы вращения по осям**  
   Для каждой пары компонент, ответственной за $x$, задаём угол:
   $$
   \theta^x_i(x) = \alpha_i \cdot x,
   $$
   для $y$:
   $$
   \theta^y_j(y) = \beta_j \cdot y,
   $$
   где $\alpha_i, \beta_j$ — частоты (как в 1D RoPE/синусоидальном кодировании).

3. **Применить два независимых поворота**  
   - На подпространстве, ассоциированном с $x$, поворачиваем пары компонент на $\theta^x_i(x)$.
   - На подпространстве, ассоциированном с $y$, поворачиваем пары компонент на $\theta^y_j(y)$.

Формально для $Q$ и $K$ в позиции $(x, y)$:
$$
Q'_{x,y} = \text{RoPE}_x(Q_{x,y}, x) \oplus \text{RoPE}_y(Q_{x,y}, y),\\
K'_{x,y} = \text{RoPE}_x(K_{x,y}, x) \oplus \text{RoPE}_y(K_{x,y}, y),
$$
где $\oplus$ — конкатенация/объединение по каналам.

#### 5.3. Почему 2D RoPE удобно для DINOv3

**Преимущества:**

- **Относительные 2D‑позиции.**  
  Attention чувствителен к **относительным смещениям** по $x$ и $y$:
  - важно не только «где токен находится», но и «насколько далеко он от другого токена по горизонтали и вертикали».

- **Масштабируемость по разрешению.**  
  При переходе на другое разрешение можно:
  - нормировать/масштабировать координаты $(x, y)$,
  - интерполировать углы $\theta^x_i, \theta^y_j$,
  не требуя жёстко зафиксированной таблицы positional embeddings под конкретное $h \times w$.

- **Сохранение пространственной структуры для dense‑задач.**  
  Поскольку позиция зашита в фазах поворотов $Q$ и $K$, а не просто добавлена как вектор, модель:
  - лучше кодирует **геометрию сцены**,
  - особенно полезна для **сегментации, детекции, depth estimation**, где точные координаты важны.

С точки зрения реализации, 2D RoPE — это:
- та же идея, что и обычный RoPE,  
- но применённая раздельно к осям $x$ и $y$ на разных подпространствах каналов.

---

### 6. Как формируются фичи: глобальные и dense‑представления

На выходе DINOv3‑ViT мы имеем:
- **глобальный вектор** (через CLS или pooling),
- **dense‑карту признаков** (через reshape патч‑токенов).

#### 6.1. Глобальные фичи

Глобальный вектор $\mathbf{z}_{\text{global}}$:
- агрегирует информацию по всем патчам,
- используется как универсальный embedding изображения.

Применения:
- **классификация** (linear probe / MLP‑head),
- **retrieval**, image‑text alignment, CLIP‑подобные задачи (при наличии текстового энкодера).

#### 6.2. Dense‑фичи (патч‑уровневые)

Патч‑токены $Z^{(L)} \in \mathbb{R}^{N \times D}$ можно reshape’ить в карту:
$$
\text{feature\_map} \in \mathbb{R}^{h \times w \times D},
$$
где каждый вектор соответствует **локальному региону** исходного изображения.

Эта карта:
- служит **бэкбоном** для:
  - object detection,
  - semantic/instance/panoptic segmentation,
  - depth/normal estimation,
  - dense matching, keypoint detection и пр.
- может быть подана в:
  - CNN‑подобный декодер (U‑Net style),
  - Transformer‑decoder (DETR/Mask2Former‑style),
  - простую upsampling‑голову.

Благодаря patch‑loss и регуляризациям, DINOv3 стремится делать эти локальные векторы:
- **богато семантическими** (распознавать объекты, части объектов),
- **устойчивыми к аугментациям**,
- и **структурированными по пространству** (за счёт 2D RoPE).

---

### 7. Применение DINOv3 для классификации

Для **классификации** типичный пайплайн:

1. **Берём предобученный DINOv3‑ViT** как encoder.
2. Получаем **глобальный embedding**:
   - либо CLS‑токен,
   - либо Global Average Pooling по патч‑токенам.
3. Добавляем небольшую **классификационную голову**:
   - самый простой вариант — линейный слой:
     $$
     \hat{y} = \text{softmax}(W_{\text{cls}}\mathbf{z}_{\text{global}} + b).
     $$
4. Обучаем:
   - либо **linear probe**: замораживаем весь ViT‑бэкбон, обучаем только $W_{\text{cls}}, b$,
   - либо полный **fine‑tuning**: дообучаем часть/все слои ViT под конкретный датасет.

Преимущество DINOv3:
- self‑supervised pretraining на огромном датасете даёт **очень хорошую линейно отделимую структуру** в пространстве фич:
  - даже простой линейный слой сверху даёт высокий accuracy.

---

### 8. Применение DINOv3 для детекции и сегментации

Для **детекции** и **сегментации** DINOv3 выступает как **универсальный визуальный бэкбон**.

#### 8.1. Детекция объектов

1. **Фича‑карта из DINOv3**
   - Прогоняем изображение через DINOv3‑ViT.
   - Берём dense‑карту признаков $h \times w \times D$ (или несколько слоёв для многоуровневых признаков).

2. **Подключаем детектор**
   Варианты:
   - **DETR‑подобные детекторы**:
     - DINOv3‑ViT даёт энкодер‑фичи,
     - декодер с набором learnable query‑токенов предсказывает боксы и классы (см.  
       `non-maximum-suppression-nms.md` и раздел про DETR в  
       `transformers-attention-and-vision-transformers-vit.md`).
   - **Anchor‑based/anchor‑free CNN‑детекторы**:
     - ViT‑фичи ресемплируются в подходящий формат (FPN‑подобная пирамида),
     - поверх строятся heads в духе Faster R‑CNN/RetinaNet/YOLO.

3. **Fine‑tuning**
   - Обычно ViT‑бэкбон **размораживают частично или полностью**,
   - детектор обучается end‑to‑end с детекционными loss‑ами (CE/Focal, IoU/GIoU и т.д.; см.  
     `detection-segmentation-3d-losses.md`).

#### 8.2. Сегментация

Для **semantic segmentation** (класс для каждого пикселя):

1. Берём dense‑карту из DINOv3‑ViT.
2. Строим **decoder**:
   - простой upsampling + несколько conv/MLP слоёв,
   - либо U‑Net/DeepLab‑style декодер,
   - либо transformer‑decoder (Mask2Former‑подобные архитектуры).
3. Проводим upsampling до исходного разрешения и считаем **softmax по классам** для каждого пикселя.

Для **instance/panoptic segmentation**:

1. Используем **query‑based схему**:
   - набор объектных query‑токенов смотрит на ViT‑фичи (cross‑attention),
   - каждый query предсказывает:
     - класс объекта,
     - маску как скалярное произведение (или небольшой MLP) между query и feature‑картой:
       $$
       \text{mask} = \sigma\big(f(\text{query}) \cdot \text{feature\_map}\big).
       $$
2. Обучаем с комбинацией:
   - классификационных loss’ов,
   - mask‑loss’ов (Dice, BCE, Lovász и др.; см. `detection-segmentation-3d-losses.md`).

**Почему DINOv3 хорошо подходит для сегментации:**
- 2D RoPE сохраняет информацию о **геометрии сцены**;
- patch‑loss и регуляризации делают патч‑фичи **богатыми и устойчивыми**;
- self‑supervised pretraining даёт **хорошую переносимость** между датасетами и задачами.

---

### 9. Связанные темы и References внутри knowledge‑book

- **Transformers, Attention and Vision Transformers (ViT)**  
  `transformers-attention-and-vision-transformers-vit.md`  
  Общий обзор трансформеров, attention, позиционного кодирования (включая RoPE и 2D‑позиции), архитектуры ViT и применение к классификации, детекции и сегментации.

- **Non-Maximum Suppression (NMS) and Modern End-to-End Detectors**  
  `non-maximum-suppression-nms.md`  
  Исторический и практический контекст object detection, классические и end‑to‑end детекторы, DETR и query‑based подходы.

- **Losses for Detection, Segmentation, and 3D Detection**  
  `detection-segmentation-3d-losses.md`  
  Подробный обзор loss‑функций, которые используются при fine‑tuning DINOv3‑бэкбона для детекции и сегментации.

- **Convolutions and Parameters in CNN**  
  `convolutions-and-parameters-in-cnn.md`  
  Полезно для сравнения: как устроены CNN‑бэкбоны по сравнению с ViT/DINOv3, какие trade‑off’ы по параметрам и вычислениям.

- **Retrieval-Augmented Generation (RAG)**  
  `retrieval-augmented-generation-rag.md`  
  Хотя тема текстовая, многие идеи attention и работы с векторными представлениями схожи с тем, как DINOv3 предоставляет универсальные визуальные эмбеддинги для downstream‑задач.

