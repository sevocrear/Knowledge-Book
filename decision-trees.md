# Деревья решений (Decision Trees)

## Как объяснить 5-летнему ребёнку

Дерево решений — это как игра «20 вопросов». Ты хочешь угадать, кто пришёл в гости. Сначала спрашиваешь: «Это мальчик или девочка?» Если мальчик — идёшь по одной ветке и спрашиваешь дальше. Если девочка — по другой. Каждый следующий вопрос разделяет людей всё точнее. В конце концов ты дойдёшь до листика и скажешь: «Это Ваня!» Деревья решений учатся задавать самые полезные вопросы, чтобы быстрее и точнее угадать ответ.

---

## Table of Contents

1. [Что такое деревья решений](#что-такое-деревья-решений)
2. [Критерии выбора разбиения (примеси)](#критерии-выбора-разбиения-примеси)
   - [Коэффициент примесей Джинни (Gini Impurity)](#коэффициент-примесей-джинни-gini-impurity)
   - [Энтропия (Entropy)](#энтропия-entropy)
   - [Прирост информации (Information Gain)](#прирост-информации-information-gain)
3. [Методы и техники построения](#методы-и-техники-построения)
4. [Ансамбли на основе деревьев](#ансамбли-на-основе-деревьев)
5. [Области применения](#области-применения)
6. [Пример кода на Python](#пример-кода-на-python)
7. [References](#references)

---

## Что такое деревья решений

**Дерево решений (Decision Tree)** — это модель машинного обучения в виде иерархической структуры «вопрос → ветвление»: в каждом узле задаётся условие по признаку, по ветвям идут объекты в зависимости от ответа, а в листьях назначается предсказание (класс или значение).

### Основные элементы

| Элемент | Описание |
|---------|----------|
| **Корень** | Верхний узел, из которого начинается дерево |
| **Внутренние узлы** | Условия вида $x_j \leq t$ или $x_j \in S$ для признака $x_j$ |
| **Ветви (рёбра)** | Пути «Да»/«Нет» или диапазоны значений |
| **Листья** | Конечные узлы с предсказанием (класс, медиана, среднее) |

### Интуиция

Дерево последовательно делит пространство признаков на области. Каждое разбиение уменьшает «неопределённость» (примесь) в подмножествах. Цель — выбрать такие вопросы (признаки и пороги), которые максимально уменьшают примесь.

---

## Критерии выбора разбиения (примеси)

Чтобы выбрать, *какой* вопрос задать в узле, нужна мера «насколько запутанно» распределение классов в узле. Чем меньше примесь — тем однороднее узлы и тем лучше разбиение.

### Коэффициент примесей Джинни (Gini Impurity)

**Индекс Джинни** $G$ — вероятность ошибиться при случайном выборе пары объектов и случайном назначении им меток по распределению в узле.

$$
G = 1 - \sum_{k=1}^{K} p_k^2
$$

где $p_k$ — доля класса $k$ в узле, $K$ — число классов.

**Свойства:**
- $G \in [0, 1]$
- $G = 0$ — узел чистый (все объекты одного класса)
- $G$ максимален при равномерном распределении ($p_k = 1/K$): $G_{max} = 1 - 1/K$

**Пример (бинарная классификация):**  
$p_1 = 0.8$, $p_2 = 0.2$:
$$G = 1 - (0.8^2 + 0.2^2) = 1 - 0.68 = 0.32$$

### Энтропия (Entropy)

**Энтропия Шеннона** $H$ измеряет среднее количество бит, нужных для кодирования класса объекта при данном распределении.

$$
H = -\sum_{k=1}^{K} p_k \log_2 p_k
$$

По соглашению: $0 \cdot \log 0 = 0$.

**Свойства:**
- $H \geq 0$
- $H = 0$ — узел чистый
- Максимум при $p_k = 1/K$: $H_{max} = \log_2 K$

**Пример (бинарная классификация):**  
$p_1 = 0.8$, $p_2 = 0.2$:
$$H = -(0.8 \log_2 0.8 + 0.2 \log_2 0.2) \approx 0.72 \text{ бит}$$

### Прирост информации (Information Gain)

**Information Gain (IG)** — это уменьшение примеси (энтропии или Джинни) при разбиении узла $S$ на подмножества $S_L$ и $S_R$ по признаку $j$ и порогу $t$:

$$
\text{IG}(S, j, t) = \text{Impurity}(S) - \frac{|S_L|}{|S|} \cdot \text{Impurity}(S_L) - \frac{|S_R|}{|S|} \cdot \text{Impurity}(S_R)
$$

где $\text{Impurity}$ — энтропия или Джинни.

**Правило выбора разбиения:** выбираем пару $(j, t)$ с **максимальным** приростом информации.

### Сравнение Gini и Entropy

| Критерий | Формула | Особенности |
|----------|---------|--------------|
| **Gini** | $1 - \sum p_k^2$ | Быстрее вычисляется, меньше логарифмов; CART по умолчанию |
| **Entropy** | $-\sum p_k \log_2 p_k$ | Имеет теоретическое обоснование в теории информации; ID3, C4.5 |

На практике оба критерия дают очень похожие деревья. Gini чаще используется из-за вычислительной простоты.

---

## Методы и техники построения

### Алгоритмы построения

| Алгоритм | Критерий разбиения | Тип задачи | Особенности |
|----------|--------------------|------------|-------------|
| **ID3** (Quinlan, 1986) | Information Gain (Entropy) | Классификация, категориальные признаки | Жадный, не обрезает переобучение |
| **C4.5** (Quinlan, 1993) | Gain Ratio (нормализованный IG) | Классификация, категориальные и числовые | Устойчив к признакам с большим числом значений |
| **CART** (Breiman et al., 1984) | Gini (классификация) или MSE (регрессия) | Классификация и регрессия | Бинарные разбиения; основа Random Forest |

### Gain Ratio (C4.5)

Чтобы снизить перекос в пользу признаков с множеством значений, в C4.5 используется **Gain Ratio**:

$$
\text{GainRatio}(S, j) = \frac{\text{IG}(S, j)}{\text{SplitInfo}(S, j)}
$$

$$
\text{SplitInfo}(S, j) = -\sum_{v \in \text{values}(j)} \frac{|S_v|}{|S|} \log_2 \frac{|S_v|}{|S|}
$$

$\text{SplitInfo}$ penalizes признаки с большим числом значений.

### Техники предотвращения переобучения

1. **Ограничение глубины** `max_depth` — не глубже $d$ уровней
2. **Минимальный размер листа** `min_samples_leaf` — в листе минимум $m$ объектов
3. **Минимум для разбиения** `min_samples_split` — не разбивать узел с числом объектов меньше порога
4. **Обрезка (pruning)**:
   - **Pre-pruning** — остановка до достижения «идеального» разбиения
   - **Post-pruning** — построить полное дерево, затем удалять узлы по критерию (например, error rate на валидации)
5. **Ограничение числа признаков** — на каждом узле рассматривать случайное подмножество признаков (используется в Random Forest)

### Специальные случаи

- **Регрессия:** в листьях предсказывается среднее (или медиана) целевой переменной, критерий разбиения — **MSE** (Mean Squared Error) или **MAE**
- **Категориальные признаки:** кодирование (one-hot, label encoding) или разбиение по подмножествам значений
- **Пропуски:** отдельная ветка «неизвестно» или отнесение к наиболее частой ветке

---

## Ансамбли на основе деревьев

Одиночные деревья склонны к переобучению и нестабильности. **Ансамбли** объединяют множество деревьев; предсказания усредняются или взвешиваются — дисперсия снижается, качество растёт.

### Bagging: Random Forest

**Bagging (Bootstrap Aggregating)** — каждая модель обучается на **бутстрэп-выборке** (случайная подвыборка с возвратом); предсказания **усредняются** (регрессия) или **голосуют** (классификация).

**Random Forest** (Breiman, 2001) — bagging деревьев CART + **random subspace**:
- на каждом узле выбирается лучший признак лишь из **случайного подмножества** признаков (обычно $\sqrt{p}$ для классификации);
- деревья строятся параллельно и независимо;
- итог: $\hat{y} = \frac{1}{B} \sum_{b=1}^{B} \hat{f}_b(x)$ или мажоритарное голосование.

Случайность по данным и признакам снижает корреляцию деревьев и переобучение.

### Boosting: AdaBoost и Gradient Boosting

**Boosting** — деревья строятся **последовательно**; каждая следующая модель исправляет ошибки предыдущих.

#### AdaBoost (Adaptive Boosting)

- На каждом шаге веса объектов пересчитываются: **ошибочно классифицированные** получают больший вес;
- Новая модель «фокус» на сложных примерах;
- Итоговое предсказание — **взвешенное голосование** по моделям.

$$
H(x) = \text{sign}\left( \sum_{t=1}^{T} \alpha_t h_t(x) \right)
$$

где $\alpha_t$ — вес $t$-го «слабого» классификатора $h_t$.

#### Gradient Boosting

Вместо весов на объектах — каждое дерево обучается на **остатках** (градиент loss-функции по текущему предсказанию):

$$
\hat{f}_{m}(x) = \hat{f}_{m-1}(x) + \eta \cdot h_m(x)
$$

$h_m$ минимизирует остаток $y - \hat{f}_{m-1}(x)$ (для MSE) или его градиент (для произвольных loss).

**Популярные реализации:**

| Библиотека | Особенности |
|------------|-------------|
| **XGBoost** | Регуляризация (L1/L2), эффективный поиск разбиений, параллелизм |
| **LightGBM** | Leaf-wise рост деревьев, histogram-based binning, быстрее на больших данных |
| **CatBoost** | Нативная работа с категориальными признаками, ordered boosting против утечки |

### Bagging vs Boosting

| Аспект | Bagging (Random Forest) | Boosting (AdaBoost, XGBoost) |
|--------|------------------------|------------------------------|
| **Обучение** | Параллельное, независимое | Последовательное, зависимое |
| **Агрегация** | Усреднение / голосование | Взвешенная сумма |
| **Фокус** | Снижение дисперсии | Снижение bias, исправление ошибок |
| **Чувствительность** | Устойчив к выбросам | AdaBoost чувствителен; GB-методы — умеренно |

---

## Области применения

| Область | Примеры |
|---------|---------|
| **Кредитный скоринг** | Оценка риска по доходу, возрасту, истории |
| **Медицина** | Диагностика по симптомам и анализам |
| **Маркетинг** | Сегментация клиентов, churn prediction |
| **Качество продукции** | Контроль производственных параметров |
| **Экспертные системы** | Правила принятия решений в бизнес-логике |
| **Interpretability** | Объяснение предсказаний (какие условия привели к решению) |
| **База для ансамблей** | Random Forest, Gradient Boosting (XGBoost, LightGBM, CatBoost) |

### Преимущества

- Интерпретируемость (видимые правила)
- Не требуют масштабирования признаков
- Работают с категориальными и числовыми признаками
- Устойчивы к выбросам по признакам (в отличие от линейных моделей)
- Нет необходимости в априорном распределении данных

### Ограничения

- Склонны к переобучению при глубоких деревьях
- Нестабильность: малое изменение данных может изменить структуру
- Плохо аппроксимируют линейные зависимости (много разбиений)
- Требуют осторожности при дисбалансе классов

---

## Пример кода на Python

```python
import numpy as np
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Загрузка данных
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Дерево с критерием Gini (CART)
clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=5)
clf_gini.fit(X_train, y_train)
print(f"Gini, accuracy: {clf_gini.score(X_test, y_test):.3f}")

# Дерево с критерием Entropy (как в ID3/C4.5)
clf_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=5)
clf_entropy.fit(X_train, y_train)
print(f"Entropy, accuracy: {clf_entropy.score(X_test, y_test):.3f}")

# Визуализация дерева
fig, ax = plt.subplots(figsize=(12, 8))
plot_tree(clf_gini, feature_names=['sepal length', 'sepal width', 'petal length', 'petal width'],
          class_names=['Setosa', 'Versicolor', 'Virginica'], filled=True, ax=ax)
plt.tight_layout()
plt.show()
```

### Random Forest и AdaBoost (sklearn)

```python
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier

# Random Forest — bagging деревьев
rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rf.fit(X_train, y_train)
print(f"Random Forest accuracy: {rf.score(X_test, y_test):.3f}")

# AdaBoost — boosting со слабыми деревьями (stumps, max_depth=1)
ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, algorithm='SAMME', random_state=42)
ada.fit(X_train, y_train)
print(f"AdaBoost accuracy: {ada.score(X_test, y_test):.3f}")
```

### Реализация Gini и Entropy вручную

```python
def gini_impurity(y: np.ndarray) -> float:
    """Коэффициент примесей Джинни."""
    _, counts = np.unique(y, return_counts=True)
    probs = counts / len(y)
    return 1 - np.sum(probs ** 2)

def entropy(y: np.ndarray) -> float:
    """Энтропия Шеннона."""
    _, counts = np.unique(y, return_counts=True)
    probs = counts / len(y)
    probs = probs[probs > 0]  # избегаем log(0)
    return -np.sum(probs * np.log2(probs))

def information_gain(y_parent, y_left, y_right, criterion='gini'):
    """Прирост информации при разбиении."""
    imp = gini_impurity if criterion == 'gini' else entropy
    n = len(y_parent)
    return imp(y_parent) - (len(y_left)/n)*imp(y_left) - (len(y_right)/n)*imp(y_right)
```

---

## References

- [Bayes' Theorem and Probability Foundations](./bayes-theorem-and-probability-foundations.md) — вероятностные основы, условная вероятность
- [ROC Curves and ROC AUC](./roc-curve-and-roc-auc.md) — метрики качества классификаторов
- [Cross Entropy and Focal Loss](./classification-losses-cross-entropy-focal-loss.md) — энтропия в loss-функциях
- Breiman, L. et al. (1984). *Classification and Regression Trees*. Wadsworth.
- Breiman, L. (2001). Random Forests. *Machine Learning*, 45(1), 5–32.
- Quinlan, J.R. (1986). Induction of Decision Trees. *Machine Learning*, 1(1), 81–106.
- Quinlan, J.R. (1993). *C4.5: Programs for Machine Learning*. Morgan Kaufmann.
- Freund, Y. & Schapire, R.E. (1997). A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. *J. Computer and System Sciences*, 55(1), 119–139.
