# Support Vector Machines (SVM) и Kernel Trick

## Как объяснить 5-летнему ребёнку

SVM — это как игра «найди самую широкую полосу между двумя кучками игрушек». Красные и синие машинки лежат на столе; ты рисуешь самую широкую полоску между ними так, чтобы ни одна машинка не залезала внутрь. Граница полоски — это «разделяющая линия». Kernel trick — это хитрость: если машинки нельзя разделить прямой линией на столе, мы мысленно подбрасываем их в воздух (переходим в пространство большей размерности), и там уже можно провести плоскость, которая их разделит, при этом все вычисления остаются такими же простыми, как для прямой на столе.

---

## Table of Contents

1. [Что такое SVM](#что-такое-svm)
2. [Линейно разделимый случай и запас (margin)](#линейно-разделимый-случай-и-запас-margin)
3. [Оптимизационная задача и двойственная форма](#оптимизационная-задача-и-двойственная-форма)
4. [Support Vectors](#support-vectors)
5. [Мягкий запас (soft margin) и параметр C](#мягкий-запас-soft-margin-и-параметр-c)
6. [Kernel Trick (ядерный трюк)](#kernel-trick-ядерный-трюк)
7. [Типичные ядра](#типичные-ядра)
8. [Пример кода на Python](#пример-кода-на-python)
9. [Связь с другими темами и применение](#связь-с-другими-темами-и-применение)
10. [References](#references)

---

## Что такое SVM

**Support Vector Machine (SVM, метод опорных векторов)** — это модель машинного обучения для классификации (и регрессии), которая ищет **гиперплоскость**, максимально отделяющую классы, с учётом **максимального запаса (margin)** до ближайших точек обучающей выборки. Точки, лежащие на границе запаса, называются **опорными векторами (support vectors)**.

### Основная идея

- Вместо того чтобы просто «разделить классы» любой линией, SVM ищет разделяющую гиперплоскость с **максимальным отступом** до обоих классов.
- Это даёт более устойчивое решение и часто лучшую обобщающую способность.
- Для **нелинейно разделимых** данных используется **kernel trick**: данные неявно отображаются в пространство большей размерности, где линейная разделимость возможна, при этом явное задание этого отображения не требуется — достаточно задать **ядро (kernel)**.

---

## Линейно разделимый случай и запас (margin)

Пусть выборка: $\mathbf{x}_i \in \mathbb{R}^d$, метки $y_i \in \{-1, +1\}$.

**Разделяющая гиперплоскость** задаётся уравнением:
$$
\mathbf{w}^\top \mathbf{x} + b = 0
$$

Решение (класс) для точки $\mathbf{x}$:
$$
\hat{y} = \operatorname{sign}(\mathbf{w}^\top \mathbf{x} + b)
$$

**Функциональный запас (functional margin)** для примера $(\mathbf{x}_i, y_i)$:
$$
\gamma_i = y_i (\mathbf{w}^\top \mathbf{x}_i + b)
$$
Он положителен, если классификация верна; его величина показывает «уверенность».

**Геометрический запас (geometric margin)** — расстояние от точки до гиперплоскости:
$$
\gamma_{\text{geom}} = \frac{\gamma_i}{\|\mathbf{w}\|} = \frac{y_i (\mathbf{w}^\top \mathbf{x}_i + b)}{\|\mathbf{w}\|}
$$

SVM максимизирует **минимальный геометрический запас** по обучающей выборке. Для корректно классифицированных точек это эквивалентно задаче:
$$
\max_{\mathbf{w}, b} \min_i \frac{y_i (\mathbf{w}^\top \mathbf{x}_i + b)}{\|\mathbf{w}\|}
$$

При фиксированном масштабе (например, $\min_i y_i(\mathbf{w}^\top \mathbf{x}_i + b) = 1$) задача сводится к **минимизации $\|\mathbf{w}\|$** при ограничениях на правильную классификацию.

---

## Оптимизационная задача и двойственная форма

### Исходная (примарная) задача

$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{при условиях} \quad y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1,\; i = 1,\ldots,n
$$

### Двойственная задача (Lagrange, по переменным $\alpha_i \geq 0$)

После введения множителей Лагранжа $\alpha_i$ и перехода к двойственной задаче получаем:

$$
\max_{\boldsymbol{\alpha}} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j
$$

при условиях $\sum_i \alpha_i y_i = 0$ и $\alpha_i \geq 0$.

Важно: в целевую функцию и в решение входят данные **только в виде скалярных произведений** $\mathbf{x}_i^\top \mathbf{x}_j$. Это ключ к kernel trick.

### Решающая функция

После решения двойственной задачи:
$$
f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b = \sum_{i \in \text{SV}} \alpha_i y_i \mathbf{x}_i^\top \mathbf{x} + b
$$

То есть веса $\mathbf{w}$ выражаются через **опорные векторы** с ненулевыми $\alpha_i$.

---

## Support Vectors

**Опорные векторы (support vectors)** — это объекты $(\mathbf{x}_i, y_i)$, для которых в решении двойственной задачи $\alpha_i > 0$. Они лежат на границе запаса ($y_i(\mathbf{w}^\top \mathbf{x}_i + b) = 1$) или внутри запаса (в случае soft margin).

- Решающая функция зависит **только** от опорных векторов.
- Все остальные точки не влияют на положение гиперплоскости.
- Это даёт разреженность (sparsity) решения и устойчивость к удалению не-опорных точек.

---

## Мягкий запас (soft margin) и параметр C

Когда данные **не линейно разделимы** или есть выбросы, жёсткие ограничения $y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1$ невыполнимы. Вводят **slack-переменные** $\xi_i \geq 0$ и ослабляют ограничения:

$$
y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i
$$

Целевая функция становится:
$$
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i
$$

**Параметр $C$** задаёт компромисс между шириной запаса и числом/величиной нарушений:
- Большой $C$ — меньше допускаем нарушений, уже запас, риск переобучения.
- Маленький $C$ — шире запас, больше допускаем ошибок на обучающей выборке, часто лучше обобщение.

В двойственной задаче ограничения на $\alpha_i$ становятся $0 \leq \alpha_i \leq C$; опорные векторы с $\alpha_i = C$ могут лежать внутри запаса или на неправильной стороне.

---

## Kernel Trick (ядерный трюк)

Если классы не разделимы линейной гиперплоскостью в исходном пространстве $\mathbb{R}^d$, их часто можно линейно разделить в пространстве **большей размерности**, куда отображаем данные: $\mathbf{x} \mapsto \phi(\mathbf{x}) \in \mathcal{H}$.

В двойственной задаче и в решающей функции данные входят только в виде **скалярных произведений**:
- $\mathbf{x}_i^\top \mathbf{x}_j$ в исходной задаче;
- $\langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle$ после отображения.

**Kernel trick**: вместо явного задания $\phi$ и вычисления $\phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j)$ в высокомерном (или бесконечномерном) пространстве задают **функцию ядра (kernel)**:
$$
k(\mathbf{x}_i, \mathbf{x}_j) = \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle
$$

Тогда все формулы SVM остаются теми же, только скалярное произведение заменяется на $k(\mathbf{x}_i, \mathbf{x}_j)$. Таким образом:
- Работаем с данными в исходном пространстве (вычисляем только $k(\mathbf{x}_i, \mathbf{x}_j)$).
- Фактически строим линейный разделитель в пространстве $\mathcal{H}$ (образы $\phi(\mathbf{x})$), что даёт нелинейную границу в исходном пространстве.

**Условие Мерсера**: функция $k$ должна быть положительно определённым ядром (symmetric, positive semi-definite), чтобы соответствовать скалярному произведению в некотором $\mathcal{H}$.

---

## Типичные ядра

| Ядро | Формула | Параметры | Назначение |
|------|---------|-----------|------------|
| **Linear** | $k(\mathbf{x}, \mathbf{z}) = \mathbf{x}^\top \mathbf{z}$ | — | Линейная граница |
| **Polynomial** | $k(\mathbf{x}, \mathbf{z}) = (\gamma \mathbf{x}^\top \mathbf{z} + r)^p$ | $\gamma, r, p$ | Полиномиальные границы |
| **RBF (Gaussian)** | $k(\mathbf{x}, \mathbf{z}) = \exp(-\gamma \|\mathbf{x} - \mathbf{z}\|^2)$ | $\gamma > 0$ | Гибкие гладкие границы, универсальный выбор |
| **Sigmoid** | $k(\mathbf{x}, \mathbf{z}) = \tanh(\gamma \mathbf{x}^\top \mathbf{z} + r)$ | $\gamma, r$ | Исторически; не всегда выполняется условие Мерсера |

**RBF (Radial Basis Function)** — одно из самых распространённых: $\gamma$ задаёт «радиус влияния» (малый $\gamma$ — более гладкая граница, большой — более извилистая, риск переобучения).

Решающая функция с ядром:
$$
f(\mathbf{x}) = \sum_{i \in \text{SV}} \alpha_i y_i\, k(\mathbf{x}_i, \mathbf{x}) + b
$$

---

## Пример кода на Python

```python
import numpy as np
from sklearn.svm import SVC
from sklearn.datasets import make_blobs, make_circles
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# Линейно разделимые данные
X_lin, y_lin = make_blobs(n_samples=100, centers=2, random_state=42)
y_lin = np.where(y_lin == 0, -1, 1)  # метки -1, +1 для наглядности

# Нелинейно разделимые (два концентрических круга)
X_circ, y_circ = make_circles(n_samples=200, noise=0.1, factor=0.4, random_state=42)
y_circ = np.where(y_circ == 0, -1, 1)

def fit_and_report(X, y, kernel='linear', C=1.0, gamma='scale'):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    clf = SVC(kernel=kernel, C=C, gamma=gamma)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    print(f"Kernel: {kernel}, C: {C}")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print(f"Number of support vectors: {len(clf.support_vectors_)}")
    print(classification_report(y_test, y_pred, target_names=['-1', '+1']))
    print()

# Линейное ядро для линейно разделимых данных
fit_and_report(X_lin, y_lin, kernel='linear')

# RBF ядро для кругов (kernel trick в действии)
fit_and_report(X_circ, y_circ, kernel='rbf', C=1.0, gamma=0.5)
```

Кратко:
- **Linear** — для линейно разделимых данных; опорных векторов обычно немного.
- **RBF** — для нелинейных границ; параметры `C` и `gamma` подбирают (например, по сетке или по кросс-валидации).

---

## Связь с другими темами и применение

- **Классическое ML**: SVM — одна из базовых моделей наряду с деревьями решений и логистической регрессией; по смыслу близка к максимизации запаса и регуляризации.
- **Метрики**: для бинарной классификации SVM часто оценивают по [ROC AUC](./roc-curve-and-roc-auc.md) и точности/полноте.
- **Ядра и эмбеддинги**: идея «скалярное произведение в пространстве признаков» перекликается с косинусной близостью в эмбеддингах и в [RAG](./retrieval-augmented-generation-rag.md).
- **Применение**: текст (часто линейное или RBF по фичам), биоинформатика, классификация изображений (до эры глубоких сетей), регрессия (SVR).

---

## References

- [Decision Trees (Деревья решений)](./decision-trees.md) — другая классическая модель классификации.
- [ROC Curves and ROC AUC](./roc-curve-and-roc-auc.md) — оценка качества бинарного классификатора.
- [Retrieval-Augmented Generation (RAG)](./retrieval-augmented-generation-rag.md) — использование векторных представлений и метрик близости.
- [Bayes' Theorem and Probability Foundations](./bayes-theorem-and-probability-foundations.md) — теоретико-вероятностные основы ML.
