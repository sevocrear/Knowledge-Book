## Batch Normalization и Layer Normalization

### Contents

1. Мотивация: зачем нужна нормализация активаций
2. Batch Normalization: идея, формулы, обучение/инференс
3. Layer Normalization: идея, формулы, где используется
4. BatchNorm vs LayerNorm: сравнение
5. Связь с обучением (learning rate, стабильность, регуляризация)
6. Как бы я объяснил это 5‑летнему ребёнку
7. References

---

### 1. Мотивация: зачем нужна нормализация активаций

При обучении глубоких сетей распределения активаций внутри слоёв постоянно меняются:

- среднее и дисперсия признаков «плывут» от шага к шагу;
- градиенты могут становиться слишком большими или слишком маленькими;
- обучение становится нестабильным и медленным.

Идея нормализации:

- на каждом слое **приводить активации к более «стандартному» виду** — примерно нулевое среднее и единичная дисперсия;
- а затем дать модели свободу через обучаемый масштаб и сдвиг (параметры `γ` и `β`).

Результат:

- более стабильные градиенты;
- возможность использовать больший learning rate;
- быстрее и устойчивее сходимость.

---

### 2. Batch Normalization

#### 2.1. Идея

Batch Normalization (BN) нормализует активации, используя статистики **по мини‑батчу**. Для CNN это делается **по каждому каналу отдельно**, усредняя по batch’у и пространственным размерам.

BN:

- вычитает среднее по батчу;
- делит на стандартное отклонение по батчу;
- затем масштабирует и сдвигает результат обучаемыми параметрами `γ` и `β`.

#### 2.2. Формулы (Conv‑вариант по каналам)

Рассмотрим один канал `c`. Для него есть значения $x_i$ из всех элементов батча и пространственных позиций. Пусть всего таких элементов $m = N \cdot H \cdot W$.

1. **Статистики по батчу:**

$$
\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i,
\qquad
\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2.
$$

2. **Нормализация:**

$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}},
$$

где $\epsilon$ — маленькая константа для численной стабильности.

3. **Обучаемый масштаб и сдвиг:**

$$
y_i = \gamma \hat{x}_i + \beta,
$$

где $\gamma$ и $\beta$ — обучаемые параметры для канала (по одному значению на канал).

Итого **на канал 2 обучаемых параметра**, а всего в слое BatchNorm — $2 \cdot C$, где $C$ — число каналов/фич.

#### 2.3. Псевдо‑алгоритм BatchNorm для батча из 5 RGB‑картинок

Рассмотрим вход в Conv‑BN блок:

- размер батча: `N = 5`;
- число каналов: `C = 3` (RGB);
- пространственный размер: `H × W`.

Тензор входа: `X` формы `(N, C, H, W) = (5, 3, H, W)`.

Псевдо‑алгоритм прямого прохода BatchNorm (режим обучения):

```text
вход: X ∈ ℝ^{5 × 3 × H × W}, параметры γ ∈ ℝ^3, β ∈ ℝ^3
выход: Y ∈ ℝ^{5 × 3 × H × W}

для каждого канала c = 0, 1, 2:
    1. Собираем все элементы этого канала по batch и пространству:
       X_c = { X[n, c, h, w] для всех n ∈ [0..4], h ∈ [0..H-1], w ∈ [0..W-1] }
       m = 5 * H * W         # количество элементов в канале

    2. Считаем среднее и дисперсию по этому множеству:
       mu_c  = (1 / m) * Σ_{n,h,w} X[n, c, h, w]
       var_c = (1 / m) * Σ_{n,h,w} (X[n, c, h, w] - mu_c)^2

    3. Нормализуем каждый элемент канала:
       для всех n, h, w:
           X_hat[n, c, h, w] = (X[n, c, h, w] - mu_c) / sqrt(var_c + ε)

    4. Применяем обучаемый масштаб и сдвиг:
       для всех n, h, w:
           Y[n, c, h, w] = γ_c * X_hat[n, c, h, w] + β_c

вернуть Y
```

При этом при обучении параллельно обновляются `running_mean[c]` и `running_var[c]` для каждого канала `c`, которые затем используются на этапе инференса.

#### 2.4. Обучение vs инференс

- **Во время обучения**:
  - используем batch‑статистики $\mu_B$ и $\sigma_B^2$;
  - параллельно ведём скользящее среднее (running mean/var) для каждого канала.

- **Во время инференса (`eval`)**:
  - статистики по текущему батчу не считаем;
  - используем сохранённые running mean и running var.

Это позволяет делать предсказания стабильно, даже при batch size = 1.

---

### 3. Layer Normalization

#### 3.1. Идея

Layer Normalization (LN) нормализует активации **внутри одного объекта**, по всем его признакам. В отличие от BN, LN **не зависит от размера и состава батча**.

Типичный пример — вектор скрытого состояния $\mathbf{h} \in \mathbb{R}^d$ (например, один токен в Transformer).

#### 3.2. Формулы

Для одного вектора $\mathbf{h} = (h_1, \dots, h_d)$:

1. **Среднее и дисперсия по признакам этого объекта:**

$$
\mu_L = \frac{1}{d} \sum_{j=1}^{d} h_j,
\qquad
\sigma_L^2 = \frac{1}{d} \sum_{j=1}^{d} (h_j - \mu_L)^2.
$$

2. **Нормализация:**

$$
\hat{h}_j = \frac{h_j - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}}.
$$

3. **Обучаемый масштаб и сдвиг (по компонентам):**

$$
y_j = \gamma_j \hat{h}_j + \beta_j.
$$

Здесь $\gamma_j$ и $\beta_j$ имеют ту же размерность, что и вектор признаков (обычно $d$).

#### 3.3. Где используется LayerNorm

- Transformers (BERT, GPT, ViT и др.);
- последовательностные модели, где batch size может быть маленьким или сильно варьироваться;
- архитектуры, где важно, чтобы поведение не зависело от размера батча (RL, онлайн‑обучение и т.п.).

---

### 4. BatchNorm vs LayerNorm: сравнение

**Ось нормализации:**

- BatchNorm:
  - нормализует **по batch’у (и пространству)**, отдельно для каждого канала;
  - статистики зависят от того, какие примеры в батче.
- LayerNorm:
  - нормализует **по признакам внутри одного объекта**, отдельно для каждого элемента batch’а;
  - статистики не зависят от размера и состава батча.

**Зависимость от batch size:**

- BatchNorm сильно зависит от размера батча:
  - при маленьких батчах статистика шумиха и может ухудшать обучение;
  - при больших батчах работает очень хорошо (классические CNN на ImageNet).
- LayerNorm устойчив к размеру батча:
  - одинаково работает при batch size = 1 и при большом batch’е;
  - поэтому стал стандартом в Transformers.

**Типовые применения:**

- BatchNorm:
  - CNN для изображений (ResNet, EfficientNet и др.);
  - как правило, используется между Conv и нелинейностью (Conv → BN → ReLU).
- LayerNorm:
  - Transformer‑блоки (до/после attention и MLP);
  - seq2seq, языковые модели, Vision Transformers.

**Обучаемые параметры:**

- В обоих случаях есть `γ` (scale) и `β` (shift), только:
  - в BN — по одному `γ` и `β` на канал;
  - в LN — по одному `γ_j` и `β_j` на каждую компоненту скрытого вектора.

---

### 5. Связь с обучением: learning rate, стабильность, регуляризация

- И BatchNorm, и LayerNorm делают распределение активаций более «упорядоченным»:
  - уменьшают внутреннее ковариантное смещение (internal covariate shift);
  - позволяют использовать **более высокий learning rate**;
  - ускоряют и стабилизируют сходимость.

- BatchNorm даёт дополнительный **регуляризующий эффект**:
  - статистики считаются по мини‑батчу → в них есть шум;
  - это немного похоже на добавление шума к активациям, что работает как регуляризация.

- LayerNorm не использует статистику по batch’у, поэтому его регуляризующий эффект меньше, но:
  - он предсказуемее при маленьких батчах;
  - лучше подходит для autoregressive‑ и sequence‑моделей.

---

### 6. Как бы я объяснил это 5‑летнему ребёнку

- Представь, что у тебя есть класс детей.
- **BatchNorm** смотрит на весь класс сразу и говорит:  
  «Вы все слишком разные по росту и весу, давайте я вас немного выровняю, чтобы было легче вас тренировать».  
  То есть он выравнивает детей **в группе**.
- **LayerNorm** смотрит на одного ребёнка и говорит:  
  «У тебя голова, руки и ноги должны быть в разумных пропорциях, я внутри тебя всё подровняю».  
  То есть он выравнивает части **внутри одного ученика**, не глядя на других.

В обоих случаях сеть потом может «настроить» масштаб и сдвиг с помощью своих ручек `γ` и `β`, если ей нужно сделать кого‑то повыше или пониже.

---

### 7. References

- **Связанные документы в этом knowledge‑book**:
  - `convolutions-and-parameters-in-cnn.md` — про свёртки и число параметров; BatchNorm в CNN обычно используется сразу после Conv.
  - `deep-reinforcement-learning.md` — в визуальных и RL‑агентах часто применяют и BatchNorm, и LayerNorm.
  - `retrieval-augmented-generation-rag.md` — использует Transformer‑модели, где LayerNorm является стандартным строительным блоком.

