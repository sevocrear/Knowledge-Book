## Table of Contents
- Объяснение для 5‑летнего
- Введение
- Что такое эмбеддинги
- Матрица эмбеддингов
- Связь с токенизатором: полный пайплайн
- Обучение эмбеддингов в LLM
- Размерность и размер словаря
- Эмбеддинги в других контекстах (RAG, поиск)
- От токенов к одному вектору на текст: pooling
- References

## Объяснение для 5‑летнего

Токенизатор даёт каждому кусочку текста **номер**. Но модель не умеет работать с номерами сами по себе — ей нужны «картинки» этих кусочков в виде точек в пространстве. **Эмбеддинг** — это как координаты такой точки: один набор чисел описывает один токен. **Матрица эмбеддингов** — это большая таблица: по номеру токена ты достаёшь из неё его координаты. Токенизатор говорит «какие номера», матрица эмбеддингов — «какие векторы по этим номерам», и уже с векторами работает сеть.

## Введение

После того как текст превращён в последовательность **ID токенов** (благодаря токенизатору), модель должна перейти от дискретных индексов к непрерывным представлениям, с которыми работают линейные слои и внимание. Эту роль выполняют **эмбеддинги** и **матрица эмбеддингов**: по каждому ID модель «достаёт» вектор фиксированной размерности. Вместе токенизатор и матрица эмбеддингов задают первый слой представления текста в любой современной LLM.

## Что такое эмбеддинги

**Эмбеддинг** (embedding) — это **вектор вещественных чисел**, который представляет какой‑то дискретный объект (токен, слово, документ, пользователя и т.д.) в непрерывном пространстве фиксированной размерности.

В контексте LLM:

- **Токенный эмбеддинг** — вектор $\mathbf{e}_i \in \mathbb{R}^d$, соответствующий токену с ID $i$.
- Размерность $d$ (например, 768, 4096, 8192) задаётся архитектурой модели и одинакова для всех токенов.
- Смысл вектора не задаётся вручную: модель **обучает** эти векторы так, чтобы по ним было удобно предсказывать следующий токен (или маскированное слово и т.п.). В результате близкие по смыслу или по контексту токены часто оказываются ближе в пространстве эмбеддингов.

Обобщённо: эмбеддинги — это способ перейти от **дискретного** (номера токенов) к **непрерывному** (векторы), чтобы дальше применять дифференцируемые операции (линейные слои, внимание, нормализацию).

## Матрица эмбеддингов

**Матрица эмбеддингов** (embedding matrix) — это параметр модели, хранящий векторы для всех токенов словаря в виде одной матрицы.

Обозначение:

$$E \in \mathbb{R}^{V \times d}$$

- $V$ — размер словаря (число токенов). Совпадает с размером словаря **токенизатора**: один индекс $i \in \{0, 1, \ldots, V-1\}$ на токен.
- $d$ — размерность эмбеддинга (hidden size модели).
- Строка $E_i$ (или столбец, в зависимости от соглашения о хранении) — это эмбеддинг токена с ID $i$.

Операция, которую выполняет модель по входной последовательности ID:

1. Дана последовательность токенов как индексы: $\mathbf{id} = (id_1, id_2, \ldots, id_n)$.
2. Для каждого $id_k$ берётся строка $E_{id_k}$.
3. Получается последовательность векторов $(\mathbf{e}_{id_1}, \mathbf{e}_{id_2}, \ldots, \mathbf{e}_{id_n})$, которую можно трактовать как тензор формы $n \times d$ (или $n \times d$ после транспонирования, в зависимости от реализации).

В коде (концептуально):

```python
# id_seq: тензор формы (batch_size, seq_len) с ID токенов
# E: матрица эмбеддингов (V, d)
embeddings = E[id_seq]   # (batch_size, seq_len, d)
```

То есть матрица эмбеддингов — это **lookup table** (таблица поиска): по целочисленному индексу возвращается вектор. Градиенты при обратном распространении обновляют только те строки $E_i$, которые реально использовались в данном батче.

## Связь с токенизатором: полный пайплайн

Связь такая:

1. **Токенизатор** (вне модели, детерминированный):
   - Вход: строка текста.
   - Выход: последовательность **токенов** (строк) и/или **ID** (целые числа из $\{0,\ldots,V-1\}$).
   - Словарь токенизатора и размер $V$ задаются при обучении/конфигурации токенизатора.

2. **Матрица эмбеддингов** (внутри модели, обучаемые параметры):
   - Вход: последовательность ID (целые числа).
   - Выход: последовательность векторов размерности $d$.
   - Размер матрицы $V \times d$ должен соответствовать словарю токенизатора: каждому ID, который может выдать токенизатор, должна соответствовать строка в $E$.

Итого:

- **Токенизатор** решает, *как* разбить текст на куски и *какие номера* им присвоить (словарь и отображение текст → ID).
- **Матрица эмбеддингов** решает, *какой вектор* по каждому такому номеру подать в сеть.

Они жёстко связаны по $V$: если токенизатор выдаёт ID из диапазона $[0, V)$, то в модели должна быть матрица эмбеддингов с $V$ строками. Несовпадение (другой токенизатор или другой $V$) приведёт к ошибкам или бессмысленному поведению.

Схема потока данных:

$$\text{Текст} \xrightarrow{\text{токенизатор}} \text{ID}_1, \ldots, \text{ID}_n \xrightarrow{E[\cdot]} \mathbf{e}_1, \ldots, \mathbf{e}_n \xrightarrow{+ \text{pos}} \text{вход в Transformer}$$

После эмбеддингов обычно добавляют позиционное кодирование (absolute, RoPE и т.д.), и полученные векторы подаются в первый слой Transformer.

## Обучение эмбеддингов в LLM

В типичной LLM эмбеддинги **не задаются вручную**: матрица $E$ инициализируется (например, случайно или по какой‑то схеме) и **обучается** вместе с остальными параметрами модели.

- При **предобучении** (language modeling) цель — предсказать следующий токен (или маскированное слово). Градиент от функции потерь обновляет в том числе строки $E_i$, соответствующие токенам в батче. Таким образом, векторы токенов подстраиваются так, чтобы представление последовательности хорошо предсказывало следующий токен.
- В декодерных моделях часто есть ещё одна матрица (например, та же $E$ или отдельная) для **выхода**: линейный слой проецирует скрытое состояние на $\mathbb{R}^V$, давая логиты по словарю. Когда выходной слой «разделяет» веса с матрицей эмбеддингов (weight tying), размерность скрытого слоя и эмбеддингов совпадает, и число параметров уменьшается.

Итог: токенизатор фиксирует *множество* токенов и их ID; матрица эмбеддингов обучается так, чтобы по этим ID давать полезные непрерывные представления для задачи модели.

## Размерность и размер словаря

- **$V$** (размер словаря):
  - Определяется **токенизатором** (BPE, WordPiece, Unigram и т.д.). Типичные значения — от нескольких тысяч до 100k–200k.
  - Чем больше $V$, тем больше параметров в матрице эмбеддингов ($V \times d$) и тем больше памяти и риска переобучения для редких токенов.

- **$d$** (размерность эмбеддинга):
  - Обычно совпадает с **hidden size** первого слоя Transformer (и часто всей модели).
  - Большее $d$ даёт модель больше «ёмкости» для представления смысла, но увеличивает объём вычислений и параметров.

Компромисс: токенизатор выбирает $V$ (и тем самым длину последовательностей и покрытие лексики); архитектура модели выбирает $d$. Оба вместе задают размер матрицы эмбеддингов и то, насколько «густо» или «разреженно» кодируется текст в начале сети.

## Эмбеддинги в других контекстах (RAG, поиск)

Понятие эмбеддинга шире, чем только токенные эмбеддинги в LLM:

- **Текстовые эмбеддинги** (для RAG, семантический поиск): отдельная модель (например, на базе BERT или Sentence-BERT) превращает целое предложение или абзац в **один** вектор. Эти векторы сравниваются по косинусному сходству или L2, чтобы находить релевантные фрагменты. Сначала модель получает по одному вектору на каждый токен (после энкодера); чтобы получить один вектор на весь текст, применяют **pooling** — см. следующий раздел.
- **Токенизатор в RAG**: документы и запросы тоже токенизуются (часто тем же семейством моделей), но эмбеддинг запроса/документа — это уже **агрегация по всем токенам** (один вектор на запрос или на чанк документа). Матрица эмбеддингов в энкодере используется так же: ID → вектор; затем по выходам энкодера для всех позиций делают pooling → один вектор на текст/чанк.

Таким образом: **матрица эмбеддингов** в LLM — это таблица «ID токена → вектор»; связь с токенизатором в том, что токенизатор определяет множество ID (словарь $V$), а матрица задаёт их непрерывное представление для входа в сеть.

## От токенов к одному вектору на текст: pooling

После энкодера (BERT, Sentence-BERT, энкодер T5 и т.п.) на выходе имеем **по одному вектору на каждый токен**: тензор формы $(n, d)$, где $n$ — длина последовательности, $d$ — размерность. Чтобы получить **один вектор на предложение или документ** (для поиска, классификации, RAG), применяют **pooling** — свёртку последовательности в один вектор размерности $d$.

Чаще всего используют следующие варианты.

### Mean pooling (усреднение)

Берётся **среднее арифметическое** по всем токенным векторам (часто только по «реальным» токенам, без [PAD]):

$$\mathbf{h}_{\text{sent}} = \frac{1}{|\mathcal{I}|} \sum_{i \in \mathcal{I}} \mathbf{h}_i,$$

где $\mathcal{I}$ — множество индексов токенов без паддинга, $\mathbf{h}_i$ — скрытое состояние (выход энкодера) на позиции $i$.

- **Плюсы**: простой, устойчивый, учитывает все токены; часто даёт хорошее качество для семантического поиска. Широко используется в Sentence-BERT и во многих эмбеддинг-моделях.
- **Минусы**: длинные документы «размазывают» смысл; стоп-слова и паддинг могут влиять (поэтому часто маскируют [PAD] при усреднении).

**Медиану** по токенам почти не используют: она менее гладкая для градиентов и не даёт устойчивого выигрыша по сравнению с mean.

### [CLS]-токен (BERT-стиль)

В BERT в начало последовательности добавляется специальный токен `[CLS]`. Выход энкодера **только на позиции [CLS]** считается представлением всего предложения:

$$\mathbf{h}_{\text{sent}} = \mathbf{h}_{[\text{CLS}]}.$$

- **Плюсы**: один вектор уже заложен в архитектуру; при предобучении BERT явно учили [CLS] кодировать смысл (например, через NSP или contrastive loss в следующих моделях).
- **Минусы**: вся информация должна «пройти» через одну позицию; для длинных текстов или моделей без специального обучения [CLS] mean pooling часто оказывается лучше.

### Last token / last hidden state (декодерные модели)

В декодер-only моделях (GPT и др.) иногда берут скрытое состояние **последнего токена** последовательности как представление текста (после прохождения через все слои). Это аналог «последнего контекста» для всей строки.

- Используется, когда нет отдельного [CLS] и нужен один вектор на последовательность; качество сильно зависит от задачи и длины контекста.

### Max pooling

По каждой размерности $j$ берётся **максимум** по всем позициям:

$$(\mathbf{h}_{\text{sent}})_j = \max_{i=1,\ldots,n} (\mathbf{h}_i)_j.$$

- Иногда применяется в комбинации с mean (например, конкатенация mean и max) для классификации или retrieval; один только max для семантического поиска используется реже.

### Кратко по практике

| Метод        | Когда типично используют                          |
|-------------|---------------------------------------------------|
| **Mean**    | Sentence-BERT, большинство текстовых эмбеддингов для RAG/поиска |
| **[CLS]**   | BERT и производные, когда модель изначально под это обучена     |
| **Last**    | Декодер-only без [CLS], когда нужен один вектор на последовательность |
| **Max**     | Дополнительный признак вместе с mean (реже отдельно)            |

Итого: для **целого предложения или документа** мы не храним все $n$ векторов, а делаем **pooling** по ним; чаще всего это **mean** (average) по непаддинг-токенам или **[CLS]**-вектор. Медиану в качестве агрегации для эмбеддингов текста практически не применяют.

## References

- Внутри `./knowledge-book`:
  - [Tokenization and Text Compression in LLMs](./tokenization-and-text-compression-in-llms.md) — откуда берутся ID токенов и как устроен словарь.
  - [Transformers, Attention and Vision Transformers (ViT)](./transformers-attention-and-vision-transformers-vit.md) — как эмбеддинги подаются в attention и слои.
  - [Retrieval-Augmented Generation (RAG)](./retrieval-augmented-generation-rag.md) — использование текстовых эмбеддингов для поиска и RAG.
