## Transformers, Attention и Vision Transformers (ViT)

### Contents

1. Введение: почему трансформеры в CV и NLP  
2. Scaled Dot-Product Attention и Q/K/V  
3. Виды Attention  
4. KV Cache (Key-Value Cache)  
5. Позиционное кодирование: зачем и какие виды бывают  
6. Архитектура Vision Transformer (ViT)  
7. Классификация, детекция и сегментация на трансформерах  
8. Связанные темы и References  
9. Как объяснить это 5‑летнему ребёнку

---

### 1. Введение: почему трансформеры в CV и NLP

Исторически:
- В **NLP** сначала доминировали RNN/LSTM/GRU, затем их вытеснили **Transformer‑модели** (BERT, GPT, T5 и т.д.).
- В **Computer Vision** долго доминировали CNN (ResNet, EfficientNet), но сейчас активно используются **Vision Transformers (ViT)** и их гибриды с CNN.

Ключевая идея трансформеров:
- Вместо последовательной обработки (как в RNN) мы даём модели **видеть весь контекст сразу** с помощью механизма **self‑attention**.
- Это хорошо масштабируется и позволяет работать с очень длинными последовательностями, большими картинками и сложными взаимосвязями.

---

### 2. Scaled Dot-Product Attention и Q/K/V

Пусть на вход слоя attention приходит матрица признаков
$$
X \in \mathbb{R}^{n \times d_{\text{model}}},
$$
где:
- $n$ — длина последовательности (количество токенов/патчей),
- $d_{\text{model}}$ — размерность эмбеддинга.

Из неё линейными проекциями получаем:
$$
Q = X W_Q,\quad
K = X W_K,\quad
V = X W_V,
$$
где $W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d_k}$ — обучаемые матрицы.

**Смысл:**
- **Query (Q)** — «что я ищу?» (запрос токена),
- **Key (K)** — «что у меня есть?» (ключ, по которому меня могут найти),
- **Value (V)** — «что я сообщу, если меня выберут?» (полезная информация).

Далее считаем **scaled dot‑product attention**:
$$
S = \frac{Q K^\top}{\sqrt{d_k}} \in \mathbb{R}^{n \times n},
$$
$$
A = \text{softmax}(S) \in \mathbb{R}^{n \times n},
$$
$$
Y = A V \in \mathbb{R}^{n \times d_k}.
$$

- $S_{ij}$ — «насколько токен $i$ должен обратить внимание на токен $j$» (похожесть $Q_i$ и $K_j$).
- после softmax каждая строка $A_i$ — распределение вероятностей по всем токенам.
- итоговый вектор $Y_i$ — **средневзвешенная сумма значений $V_j$**, где веса определяет похожесть запросов и ключей.

**Multi-Head Attention**:
- вместо одного набора $(W_Q, W_K, W_V)$ берём $h$ разных голов с меньшей размерностью,
- каждая голова фокусируется на разных аспектах структуры (лексика, синтаксис, длинные зависимости и т.п.),
- выходы голов конкатенируются и ещё раз проецируются линейным слоем.

---

### 3. Виды Attention

#### 3.1. По направлению «кто на кого смотрит»

- **Self-Attention**  
  - Query, Key и Value берутся из **одной и той же** последовательности $X$.
  - Каждый токен может смотреть на все остальные токены.
  - Используется в энкодерах (BERT, ViT) и декодерах (GPT).

- **Cross-Attention**  
  - Query берутся из одной последовательности, Key/Value — из другой.  
  - Примеры:
    - декодер seq2seq‑модели смотрит на выход энкодера;
    - в DETR объектные query‑токены смотрят на визуальные фичи.

- **Encoder–Decoder Attention**  
  - Частный случай cross‑attention в оригинальном Transformer.

#### 3.2. По причинности (маскированию)

- **Bidirectional Attention**  
  - Каждый токен видит все другие (прошлые и будущие),
  - используется в BERT, ViT и большинстве энкодеров.

- **Causal / Masked Attention**  
  - Токен на позиции $t$ видит только токены $1,\dots,t$,
  - реализуется с помощью маски в матрице $S$ (запрещаем смотреть в будущее),
  - используется в GPT‑подобных языковых моделях и autoregressive генерации.

#### 3.3. По структуре (full / sparse / локальный)

- **Full Attention**  
  - Каждый токен видит все токены, сложность $O(n^2)$ по длине.

- **Local / Sliding Window Attention**  
  - Каждый токен видит только соседей (окно размером $w$),
  - подходит для очень длинных последовательностей (аудио, лог‑файлы).

- **Sparse Attention**  
  - Заранее заданный или обученный шаблон разреженности:
    - часть токенов — глобальные (видят всех),
    - остальные — локальные.
  - Примеры: Longformer, BigBird, Sparse Transformer.

- **Linear / Efficient Attention**  
  - Изменяем формулу attention с помощью ядерных трюков / низкоранговых аппроксимаций,
  - добиваемся сложности ближе к $O(n)$,
  - Примеры: Performer, Linformer, многие state space‑модели (Mamba) решают ту же задачу «эффективных длинных контекстов».

#### 3.4. По организации голов

- **Multi-Head Attention (MHA)**  
  - Классический вариант: каждая голова со своими $W_Q^h, W_K^h, W_V^h$.

- **Multi-Query Attention (MQA)**  
  - Разные Queries на головах, но **общие** $K, V$,
  - значительно уменьшает размер KV‑кэша и ускоряет инференс.

- **Grouped-Query Attention (GQA)**  
  - Компромисс между MHA и MQA: несколько групп голов разделяют $K, V$.

---

### 4. KV Cache (Key-Value Cache)

**Проблема без кэша:**
- В autoregressive‑генерации на шаге $t$ нам нужно attention токена $t$ ко всем предыдущим токенам $1,\dots,t$.
- Наивно каждый раз пересчитывать $K_{1:t}$ и $V_{1:t}$ для всех слоёв дорого: повторяем одну и ту же работу.

**Идея KV‑кэша:**
- Во время генерации **сохраняем** рассчитанные $K$ и $V$ для каждого слоя и всех уже обработанных токенов.
- На шаге $t$:
  - считаем $Q_t$ (и $K_t, V_t$ для текущего токена),
  - конкатенируем их к кэшированным $K_{1:t-1}, V_{1:t-1}$,
  - считаем attention только между $Q_t$ и уже сохранёнными $K, V$.

**Результат:**
- сложность по времени на шаг генерации становится **линейной по длине контекста** только для текущего шага,
- не нужно повторно прогонять всю последовательность через все слои,
- это ключевая оптимизация при инференсе LLM‑ов в проде.

---

### 5. Позиционное кодирование: зачем и какие виды бывают

**Зачем нужно позиционное кодирование?**
- Базовый self‑attention инвариант к перестановкам токенов: множество, а не последовательность.
- Язык, аудио и изображение — это **структурированные данные**, где порядок и расстояние важны.
- Позиционное кодирование добавляет информацию о положении/структуре в векторы токенов:
$$
\tilde{\mathbf{x}}_i = \mathbf{x}_i + \mathbf{p}_i.
$$

#### 5.1. Абсолютное позиционное кодирование

- **Синусоидальное (original Transformer)**:
  - Для позиции $pos$ и измерения $2i$:
    $$
    PE_{pos, 2i}   = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right),\quad
    PE_{pos, 2i+1} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right).
    $$
  - Необучаемое, позволяет обобщать на длины больше обучающих.

- **Обучаемое absolute positional embedding**:
  - Просто таблица эмбеддингов по индексам позиций, параметры обучаются.
  - В ViT часто используют **2D‑позиции** (строка, столбец патча).

#### 5.2. Относительное позиционное кодирование

- Позиция определяется **не самой по себе**, а через разность $(i - j)$ между токенами.
- В attention добавляется дополнительный член, зависящий от разности позиций:
  - например, в матрицу $S = QK^\top / \sqrt{d_k}$ добавляют bias, зависящий от $(i - j)$.
- Лучше моделирует локальность и переносимость между разными длинами.

#### 5.3. Rotary Positional Embeddings (RoPE)

- Вместо прибавления позиционного вектора к эмбеддингу, мы **поворачиваем** пары компонент векторов $Q$ и $K$ на угол, зависящий от позиции.
- Это обеспечивает:
  - естественное кодирование относительных сдвигов,
  - хорошие свойства для extrapolation на большие длины контекста,
  - часто используется в современных LLM (LLaMA, Qwen и др.).

#### 5.4. ALiBi и другие bias‑подходы

- **ALiBi (Attention with Linear Biases)**:
  - Вместо хранения таблицы позиций вводится **линейный штраф** в attention за расстояние между токенами.
  - Уменьшает память и хорошо масштабируется на длинные последовательности.

#### 5.5. 2D/3D позиционное кодирование для CV

- Для изображений и видео нужно кодировать не только порядок, но и **координаты в пространстве**:
  - $(x, y)$ для изображений,
  - $(x, y, t)$ для видео.
- Распространённые варианты:
  - раздельные эмбеддинги по осям: `row_emb[x] + col_emb[y]`,
  - общий эмбеддинг для пары $(x, y)$,
  - 2D‑версия синусоидального/rotary кодирования.

---

### 6. Архитектура Vision Transformer (ViT)

**Вход**: изображение размера $H \times W \times C$.

1. **Разбиение на патчи**
   - Режем изображение на патчи размера $P \times P$.
   - Получаем $\frac{H}{P} \cdot \frac{W}{P}$ патчей (предполагаем, что $H, W$ кратны $P$).

2. **Линейное отображение патчей**
   - Каждый патч разворачиваем в вектор длины $P^2 \cdot C$.
   - Умножаем на матрицу $W \in \mathbb{R}^{(P^2 C) \times D}$ и получаем эмбеддинг размерности $D$.
   - Итого: последовательность $\mathbf{x}_1, \dots, \mathbf{x}_N$, где $N = \frac{H}{P} \cdot \frac{W}{P}$.

3. **Добавление CLS‑токена**
   - В начало последовательности добавляем learnable‑вектор $\mathbf{x}_{\text{CLS}} \in \mathbb{R}^D$.
   - Его задача — агрегировать глобальную информацию о картинке.

4. **Позиционное кодирование**
   - К каждому токену (включая CLS) прибавляем позиционный вектор:
     - либо 1D по индексу патча,
     - либо 2D по координатам патча.

5. **Проход через энкодер трансформера**
   - Несколько одинаковых блоков:
     - Multi-Head Self-Attention,
     - MLP (feedforward),
     - Residual connections,
     - LayerNorm.

6. **Голова классификации**
   - На выходе берём вектор $\mathbf{z}_{\text{CLS}}$ (обновлённый CLS‑токен),
   - подаём его в линейный слой:
     $$
     \hat{y} = \text{softmax}(W_{\text{cls}} \mathbf{z}_{\text{CLS}}).
     $$

---

### 7. Классификация, детекция и сегментация на трансформерах

#### 7.1. Классификация

Основные подходы:

- **С CLS‑токеном**:
  - Добавляем CLS в начало последовательности.
  - Прогоняем через $L$ слоёв трансформера.
  - Берём последний CLS и подаём в MLP на классы.

- **Без CLS‑токена**:
  - делаем `mean pooling`/`max pooling` по всем токенам,
  - или attention‑pooling (отдельный маленький attention, который учится «выбирать» важные токены).

В ViT‑подобных архитектурах CLS‑вариант — наиболее распространённый.

#### 7.2. Детекция (обнаружение объектов)

Ключевой пример — **DETR (Detection Transformer)** и его производные:

1. **Backbone** (CNN или ViT) извлекает карту признаков изображения.
2. **Энкодер трансформера** обрабатывает карту признаков (последовательность патчей/feature‑токенов).
3. **Декодер трансформера с объектными query‑токенами**:
   - есть $M$ learnable‑query, каждый потенциально соответствует одному объекту,
   - в cross‑attention они «смотрят» на визуальные признаки (ключи и значения из энкодера).
4. Для каждого query декодер выдаёт:
   - класс объекта (или `no-object`),
   - координаты бокса (обычно нормализованный центр, ширина и высота).
5. Используется **Hungarian matching** для сопоставления предсказанных боксов и разметки.

Альтернативный путь:
- взять ViT‑фичи как 2D‑карту и строить поверх них **anchor‑based или anchor‑free детектор** (как Faster R‑CNN/RetinaNet/YOLO, но уже не на CNN‑фичах, а на ViT‑фичах).

#### 7.3. Сегментация

**Semantic segmentation (класс для каждого пикселя)**:

1. Используем ViT как encoder (feature extractor).
2. Преобразуем последовательность патчей обратно в 2D карту.
3. Добавляем decoder (up‑sampling, skip‑connections, либо transformer‑decoder), который восстанавливает пространственное разрешение.
4. На каждом пикселе/патче считаем softmax по классам.

**Instance и Panoptic segmentation** (Mask2Former, Mask DINO и др.):

- Идея очень похожа на DETR:
  - есть набор query‑токенов, каждый отвечает за один объект/регион,
  - каждый query предсказывает:
    - класс (или тип сегмента: «объект», «фон»),
    - маску (обычно как attention/inner‑product query к feature‑карте).
- Маска часто вычисляется как:
  $$
  \text{mask} = \sigma\big(f(\text{query}) \cdot \text{feature\_map}\big),
  $$
  где $f$ — небольшая голова (MLP/conv), а `feature_map` — пространственные фичи энкодера.

---

### 8. Связанные темы и References

- **Convolutions and Parameters in CNN**  
  - Хорошо понимать CNN и свёртки, прежде чем переходить к ViT и DETR.

- **Non-Maximum Suppression (NMS) and Modern End-to-End Detectors**  
  - Связано с детекцией, особенно с переходом от NMS к end‑to‑end детекторам на трансформерах (DETR, RT‑DETR).

- **Batch Normalization and Layer Normalization**  
  - LayerNorm — ключевой компонент архитектуры трансформеров (ViT, LLM).

- **Retrieval-Augmented Generation (RAG)**  
  - Использует attention и кросс‑attention между запросом и документами; концептуально близко к cross‑attention в DETR и ViT‑подобных архитектурах.

- **Low-Rank Adaptation (LoRA)**  
  - Эффективная техника тонкой настройки Transformer моделей (LLM, ViT) через добавление низкоранговых адаптаций к attention и MLP слоям.

- **DINOv3: Self-Supervised Vision Transformer и 2D RoPE**  
  - Отдельный документ с углублённым разбором self‑supervised обучения ViT‑бэкбонов (student–teacher, multi‑view, multi‑loss) и 2D RoPE для изображений, а также практическим применением DINOv3‑фич для классификации, детекции и сегментации.

---

### 9. Как объяснить это 5‑летнему ребёнку

- **Трансформер** — это как класс, где каждый ученик может слушать всех остальных и решает, кто сейчас говорит что‑то важное.  
- **Q, K, V** — у каждого ученика есть вопрос (Q), бейджик с описанием (K) и история, которой он может поделиться (V). Ученик слушает тех, у кого бейджик лучше всего подходит к его вопросу.  
- **Attention** — это способ решить, кого слушать сильнее, а кого тише.  
- **Позиционное кодирование** — мы добавляем к каждому ученику номер стула или место за партой, чтобы не перепутать, кто где сидит и в каком порядке говорит.  
- **KV‑кэш** — это как тетрадка, куда класс уже записал всё, что сказал раньше, чтобы больше не повторять и не тратить время.  
- **ViT** — мы режем картинку на маленькие квадратики, как пазл, каждый квадратик становится «учеником», и все они обсуждают друг друга, чтобы понять, что нарисовано. В начале сидит особенный ученик (CLS), который потом рассказывает учителю итог: «на картинке кот/собака/машина».  
- **Детекция и сегментация** — это когда ученики не только говорят «здесь есть кот», но и показывают, где он на картинке (рамка) и раскрашивают его пиксели отдельным цветом.

